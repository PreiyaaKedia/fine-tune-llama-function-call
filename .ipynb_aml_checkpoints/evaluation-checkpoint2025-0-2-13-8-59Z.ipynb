{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!az login --tenant fdpo.onmicrosoft.com"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import the pip dependencies\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "import sys\n",
        "# !{sys.executable} -m pip install azure-ai-evaluation\n",
        "!{sys.executable} -m pip install azure-ai-evaluation[remote]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735802020531
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"./slm_perf_output.csv\")\n",
        "\n",
        "sample = dataset.iloc[0]\n",
        "print(sample)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Prompt      [{'role': 'system', 'content': 'You are a help...\nActual      {'tool_uses': [{'recipient_name': 'functions.s...\nExpected    {'tool_uses': [{'recipient_name': 'functions.s...\nMatch                                                      No\nName: 0, dtype: object\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735797240241
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "from azure.ai.evaluation import BleuScoreEvaluator, GleuScoreEvaluator, MeteorScoreEvaluator, RougeScoreEvaluator, RougeType\n",
        "\n",
        "bleu = BleuScoreEvaluator()\n",
        "glue = GleuScoreEvaluator()\n",
        "meteor = MeteorScoreEvaluator(alpha = 0.9, beta = 3.0, gamma = 0.5)\n",
        "rouge = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_L)\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'custom_evaluators'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BleuScoreEvaluator, GleuScoreEvaluator, MeteorScoreEvaluator, RougeScoreEvaluator, RougeType\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_evaluators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdifference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DifferenceEvaluator\n\u001b[1;32m      5\u001b[0m bleu \u001b[38;5;241m=\u001b[39m BleuScoreEvaluator()\n\u001b[1;32m      6\u001b[0m glue \u001b[38;5;241m=\u001b[39m GleuScoreEvaluator()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'custom_evaluators'"
          ]
        }
      ],
      "execution_count": 61,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735814175519
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./llama-fc_config.yaml') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_SFT_DATA_NAME']    \n",
        "DATA_DIR = d['config']['SFT_DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735802487811
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./slm_perf_output.csv\")\n",
        "df.rename(columns = {\"Actual\" : \"ground_truth\", \"Expected\" : \"response\"}, inplace = True)\n",
        "df.to_json(\"./eval_baseline_data.jsonl\", orient = \"records\", lines = True)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735797266180
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# <mySubscriptionID>: Subscription ID of the Azure AI Studio hub's linked storage account (available in Azure AI hub resource view in Azure Portal).\n",
        "# <myResourceGroupName>: Resource group of the Azure AI Studio hub's linked storage account.\n",
        "# <user-id>: User object ID for role assignment (retrieve with \"az ad user show\" command).\n",
        "\n",
        "! az role assignment create --role \"Storage Blob Data Contributor\" --scope /subscriptions/8cebb108-a4d5-402b-a0c4-f7556126277f/resourceGroups/openairesourcegroup_priya --assignee-principal-type User --assignee-object-id \"8b6b15cc-9937-4208-8173-97ea0b9e1a13\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate(\n",
        "    data=\"./eval_baseline_data.jsonl\",\n",
        "    evaluators={\n",
        "        \"bleu\": bleu,\n",
        "        \"gleu\": glue,\n",
        "        \"meteor\": meteor,\n",
        "        \"rouge\" : rouge\n",
        "    },\n",
        "    # Optionally provide your AI Studio project information to track your evaluation results in your Azure AI Studio project\n",
        "    azure_ai_project={\n",
        "        \"subscription_id\": \"8cebb108-a4d5-402b-a0c4-f7556126277f\",\n",
        "        \"resource_group_name\": \"openAIResourceGroup_priya\",\n",
        "        \"project_name\": \"priyakedia-1214\",\n",
        "    },\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Prompt flow service has started...\nPrompt flow service has started...\nPrompt flow service has started...\nYou can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_bleu_bleu_asyncbleuscoreevaluator_ybi6iz8s_20250102_084844_994039\nYou can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_gleu_gleu_asyncgleuscoreevaluator_76w_hv1i_20250102_084845_003956\nYou can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=azure_ai_evaluation_evaluators_meteor_meteor_asyncmeteorscoreevaluator_ne3uinm9_20250102_084844_999574\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735807733440
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(result[\"rows\"])\n",
        "\n",
        "# df\n",
        "\n",
        "# sample_df = df.loc[df[\"inputs.ground_truth\"].str.startswith(\"{'tool_uses'}\")]\n",
        "# sample_df = df.loc[df[\"inputs.ground_truth\"].str.startswith(\"{'tool_uses'}\")]  \n",
        "\n",
        "# sample_df\n",
        "\n",
        "index = 19\n",
        "\n",
        "# df.iloc[index][\"inputs.ground_truth\"]\n",
        "df.iloc[index][\"inputs.response\"]\n",
        "\n",
        "# print(\"\\n\", df.iloc[0][\"inputs.response\"])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 57,
          "data": {
            "text/plain": "\"{'tool_uses': [{'recipient_name': 'functions.calculate_age', 'parameters': {'birthdate': '1990-05-15'}}]}\""
          },
          "metadata": {}
        }
      ],
      "execution_count": 57,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735810348331
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.evaluation import GleuScoreEvaluator\n",
        "\n",
        "gleu = GleuScoreEvaluator()"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735590790408
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}