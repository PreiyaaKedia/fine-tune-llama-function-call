{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Serving Preparation\n",
        "##### \n",
        "##### 1.1 Configure Workspace details\n",
        "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "with open('./llama-fc_config.yaml') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_SFT_DATA_NAME']    \n",
        "DATA_DIR = d['config']['SFT_DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
        "IS_DEBUG = d['config']['IS_DEBUG']\n",
        "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
        "\n",
        "azure_env_name = d['serve']['azure_env_name']\n",
        "azure_compute_cluster_name = d['serve']['azure_compute_cluster_name']\n",
        "azure_compute_cluster_size = d['serve']['azure_serving_cluster_size']\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CLOUD_DIR, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1735288089736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create console handler with a higher log level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create file handler which logs even debug messages\n",
        "file_handler = logging.FileHandler(\"debug.log\")\n",
        "file_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(console_handler)\n",
        "logger.addHandler(file_handler)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288091402
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
        "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
        "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
        "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
        "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
        "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
        "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
        "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
        "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
        "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
        "logger.info(f\"azure_env_name={azure_env_name}\")\n",
        "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
        "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-12-27 08:28:13,939 - __main__ - INFO - ===== 0. Azure ML Training Info =====\n2024-12-27 08:28:13,969 - __main__ - INFO - AZURE_SUBSCRIPTION_ID=8cebb108-a4d5-402b-a0c4-f7556126277f\n2024-12-27 08:28:13,976 - __main__ - INFO - AZURE_RESOURCE_GROUP=azure-ml-priya-demo\n2024-12-27 08:28:13,981 - __main__ - INFO - AZURE_WORKSPACE=azure-ml-priya-westus3\n2024-12-27 08:28:13,987 - __main__ - INFO - AZURE_DATA_NAME=sft-demo-data-function-call\n2024-12-27 08:28:13,992 - __main__ - INFO - DATA_DIR=./dataset\n2024-12-27 08:28:13,997 - __main__ - INFO - CLOUD_DIR=./cloud\n2024-12-27 08:28:14,002 - __main__ - INFO - HF_MODEL_NAME_OR_PATH=unsloth/Llama-3.2-3B-Instruct\n2024-12-27 08:28:14,007 - __main__ - INFO - IS_DEBUG=True\n2024-12-27 08:28:14,012 - __main__ - INFO - USE_LOWPRIORITY_VM=False\n2024-12-27 08:28:14,018 - __main__ - INFO - azure_env_name=slm-serving-llama\n2024-12-27 08:28:14,023 - __main__ - INFO - azure_compute_cluster_name=gpu-a100-demo-vm\n2024-12-27 08:28:14,028 - __main__ - INFO - azure_compute_cluster_size=Standard_NC24ads_A100_v4\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288093494
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288098362
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.2 Create Model asset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Run\n",
        "\n",
        "def get_or_create_model_asset(ml_client, model_name, experiment_name = None, job_name = None, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
        "        else:\n",
        "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
        "            print(f\"Found Model asset: {model_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\") \n",
        "        ws = Workspace.from_config()  \n",
        "  \n",
        "        # Get the run by its ID   \n",
        "        run = Run(ws.experiments[experiment_name], job_name)  \n",
        "        # Register the model  \n",
        "        model_asset = run.register_model(  \n",
        "            model_name=model_name,  # this is the name the model will be registered under  \n",
        "            model_path=model_dir  # this is the path to the model file in the run's outputs  \n",
        "        )         \n",
        "        print(f\"Created Model asset: {model_name}\")\n",
        "\n",
        "    return model_asset"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288514090
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ml_client.models.get(d['serve']['azure_model_name'], version=\"latest\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceNotFoundError",
          "evalue": "(UserError) The specified resource was not found.\nCode: UserError\nMessage: The specified resource was not found.\nException Details:\t(NoSuchModelRegistered) There is no registered model in Account Subscription: 8cebb108-a4d5-402b-a0c4-f7556126277f, ResourceGroup: azure-ml-priya-demo, Workspace: azure-ml-priya-westus3 with id llama-fc-ft:latest\n\tCode: NoSuchModelRegistered\n\tMessage: There is no registered model in Account Subscription: 8cebb108-a4d5-402b-a0c4-f7556126277f, ResourceGroup: azure-ml-priya-demo, Workspace: azure-ml-priya-westus3 with id llama-fc-ft:latest",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserve\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mazure_model_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlatest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:361\u001b[0m, in \u001b[0;36mModelOperations.get\u001b[0;34m(self, name, version, label)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationException(\n\u001b[1;32m    354\u001b[0m         message\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m    355\u001b[0m         target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mMODEL,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    358\u001b[0m         error_type\u001b[38;5;241m=\u001b[39mValidationErrorType\u001b[38;5;241m.\u001b[39mMISSING_FIELD,\n\u001b[1;32m    359\u001b[0m     )\n\u001b[1;32m    360\u001b[0m \u001b[38;5;66;03m# TODO: We should consider adding an exception trigger for internal_model=None\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m model_version_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Model\u001b[38;5;241m.\u001b[39m_from_rest_object(model_version_resource)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:307\u001b[0m, in \u001b[0;36mModelOperations._get\u001b[0;34m(self, name, version)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, version: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelVersion:  \u001b[38;5;66;03m# name:latest\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version:\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_versions_operation\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    301\u001b[0m                 name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    302\u001b[0m                 version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    303\u001b[0m                 registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name,\n\u001b[1;32m    304\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs,\n\u001b[1;32m    305\u001b[0m             )\n\u001b[1;32m    306\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m--> 307\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_versions_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m                \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scope_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m         )\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_container_operation\u001b[38;5;241m.\u001b[39mget(name\u001b[38;5;241m=\u001b[39mname, registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs)\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    320\u001b[0m         )\n\u001b[1;32m    321\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_08_01_preview/operations/_model_versions_operations.py:571\u001b[0m, in \u001b[0;36mModelVersionsOperations.get\u001b[0;34m(self, resource_group_name, workspace_name, name, version, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m]:\n\u001b[0;32m--> 571\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/exceptions.py:161\u001b[0m, in \u001b[0;36mmap_error\u001b[0;34m(status_code, response, error_map)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    160\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
            "\u001b[0;31mResourceNotFoundError\u001b[0m: (UserError) The specified resource was not found.\nCode: UserError\nMessage: The specified resource was not found.\nException Details:\t(NoSuchModelRegistered) There is no registered model in Account Subscription: 8cebb108-a4d5-402b-a0c4-f7556126277f, ResourceGroup: azure-ml-priya-demo, Workspace: azure-ml-priya-westus3 with id llama-fc-ft:latest\n\tCode: NoSuchModelRegistered\n\tMessage: There is no registered model in Account Subscription: 8cebb108-a4d5-402b-a0c4-f7556126277f, ResourceGroup: azure-ml-priya-demo, Workspace: azure-ml-priya-westus3 with id llama-fc-ft:latest"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288467561
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Model(\n",
        "#     name=d['serve']['azure_model_name'],        \n",
        "#     path=\"./model/outputs\",\n",
        "#     description=\"Model created from run.\",\n",
        "#     type=\"custom_model\" # mlflow_model, custom_model, triton_model\n",
        "#     )"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735024615554
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_or_create_model_asset(ml_client, d['serve']['azure_model_name'], update = False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found Model asset: llama-fc-ft. Will not create again\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288518315
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Create AzureML environment\n",
        "####\n",
        "Azure ML defines containers (called environment asset) in which your code will run. You can use a pre-built enviornment or create a custom enviornment. For this hands-on session, we will buid a custom Docker enviornment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/serve/Dockerfile\n",
        "\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "\n",
        "# Inference requirements\n",
        "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
        "\n",
        "RUN /var/requirements/install_system_requirements.sh && \\\n",
        "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
        "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
        "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
        "    rm -f /etc/nginx/sites-enabled/default\n",
        "ENV SVDIR=/var/runit\n",
        "ENV WORKER_TIMEOUT=400\n",
        "EXPOSE 5001 8883 8888\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y openssh-server openssh-client\n",
        "\n",
        "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/serve/Dockerfile\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/serve/requirements.txt\n",
        "azureml-core==1.58.0\n",
        "azureml-dataset-runtime==1.58.0\n",
        "azureml-defaults==1.58.0\n",
        "azure-ml==0.0.1\n",
        "azure-ml-component==0.9.18.post2\n",
        "azureml-mlflow==1.58.0\n",
        "azureml-contrib-services==1.58.0\n",
        "azureml-contrib-services==1.58.0\n",
        "azureml-automl-common-tools==1.58.0\n",
        "torch-tb-profiler==0.4.3\n",
        "azureml-inference-server-http~=1.3\n",
        "inference-schema==1.8.0\n",
        "MarkupSafe==3.0.2\n",
        "regex\n",
        "pybind11\n",
        "bitsandbytes==0.44.1\n",
        "transformers==4.46.1\n",
        "peft==0.13.2\n",
        "accelerate==1.1.0\n",
        "datasets\n",
        "scipy\n",
        "azure-identity\n",
        "packaging==24.1\n",
        "timm==1.0.11\n",
        "einops"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/serve/requirements.txt\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        env_docker_image = Environment(\n",
        "            build=BuildContext(path=docker_dir),\n",
        "            name=env_name,\n",
        "            description=\"Environment created from a Docker context.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "    \n",
        "    return env_asset\n",
        "\n",
        "env = get_or_create_docker_environment_asset(ml_client, azure_env_name, f\"{CLOUD_DIR}/serve\", update=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Exception: (UserError) System.Net.Http.HttpConnectionResponseContent\nCode: UserError\nMessage: System.Net.Http.HttpConnectionResponseContent\nCreated Environment asset: slm-serving-florence\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading serve (0.0 MBs): 100%|██████████| 1720/1720 [00:00<00:00, 60485.81it/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735020878784
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Serving script\n",
        "####\n",
        "If you are not serving using a MLflow model but instead using a custom model, you can write your own script. This step demosntrates how to write the scoring script to run the inference.\n",
        "\n",
        "The scoring script consists of two components:\n",
        "\n",
        "1. init() : This is where you define the global initialization logic like loading of LLM models and tokenizers\n",
        "2. run() : Inference logic called for every invocation of the endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/inference/score.py\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import base64\n",
        "import logging\n",
        "\n",
        "from io import BytesIO\n",
        "from transformers import AutoTokenizer, AutoProcessor, pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    global tokenizer\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # Please provide your model's folder name if there is one\n",
        "    model_name_or_path = os.path.join(\n",
        "        os.getenv(\"AZUREML_MODEL_DIR\"), \"outputs\"\n",
        "    )\n",
        "    \n",
        "    model_kwargs = dict(\n",
        "        trust_remote_code=True,    \n",
        "        device_map={\"\":0},\n",
        "        torch_dtype=\"auto\" \n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map ={\"\" : 0}, **model_kwargs)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)    \n",
        "\n",
        "    logging.info(\"Loaded model.\")\n",
        "    \n",
        "def run(json_data: str):\n",
        "    logging.info(\"Request received\")\n",
        "    data = json.loads(json_data)\n",
        "    input_data = data[\"input_data\"]\n",
        "    params = data['params']\n",
        "\n",
        "    pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
        "    output = pipe(input_data, **params)\n",
        "    result = output[0][\"generated_text\"]\n",
        "    logging.info(f\"Generated text : {result}\")\n",
        "    json_result = {\"result\" : str(result)}\n",
        "\n",
        "    return json_result"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./cloud/serve/score.py\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Serving\n",
        "#####\n",
        "##### 2.1 Create the endpoint\n",
        "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model.\n",
        " \n",
        "Note : This step doesn't provision the GPU"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    IdentityConfiguration,\n",
        "    ManagedIdentityConfiguration,\n",
        ")\n",
        "\n",
        "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
        "# Check if the endpoint already exists in the workspace\n",
        "try:\n",
        "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
        "    print(\"---Endpoint already exists---\")\n",
        "except:\n",
        "    # Create an online endpoint if it doesn't exist\n",
        "\n",
        "    # Define the endpoint\n",
        "    endpoint = ManagedOnlineEndpoint(\n",
        "        name=azure_endpoint_name,\n",
        "        description=f\"Test endpoint for {model.name}\",\n",
        "        # identity=IdentityConfiguration(\n",
        "        #     type=\"user_assigned\",\n",
        "        #     user_assigned_identities=[ManagedIdentityConfiguration(resource_id=uai_id)],\n",
        "        # )\n",
        "        # if uai_id != \"\"\n",
        "        # else None,\n",
        "    )\n",
        "\n",
        "# Trigger the endpoint creation\n",
        "try:\n",
        "    ml_client.begin_create_or_update(endpoint).wait()\n",
        "    print(\"\\n---Endpoint created successfully---\\n\")\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Endpoint creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n---Endpoint created successfully---\n\nCPU times: user 65.3 ms, sys: 4.39 ms, total: 69.7 ms\nWall time: 1min 33s\n"
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Create the Deployment\n",
        "This process takes lot of time as GPU clusters needs to be provisioned and serving environment must be built"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from azure.ai.ml.entities import (    \n",
        "    OnlineRequestSettings,\n",
        "    CodeConfiguration,\n",
        "    ManagedOnlineDeployment,\n",
        "    ProbeSettings,\n",
        "    Environment\n",
        ")\n",
        "\n",
        "azure_deployment_name = d['serve']['azure_deployment_name']\n",
        "\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=azure_deployment_name,\n",
        "    endpoint_name=azure_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=azure_compute_cluster_size,\n",
        "    instance_count=1,\n",
        "    #code_configuration=code_configuration,\n",
        "    environment = env,\n",
        "    scoring_script=\"score.py\",\n",
        "    code_path=f\"./{CLOUD_DIR}/inference\",\n",
        "    #environment_variables=deployment_env_vars,\n",
        "    request_settings=OnlineRequestSettings(max_concurrent_requests_per_instance=20,\n",
        "                                           request_timeout_ms=90000, max_queue_wait_ms=60000),\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        period=100,\n",
        "        initial_delay=500,\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        period=100,\n",
        "        initial_delay=500,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Trigger the deployment creation\n",
        "try:\n",
        "    ml_client.begin_create_or_update(deployment).wait()\n",
        "    print(\"\\n---Deployment created successfully---\\n\")\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err\n",
        "    \n",
        "endpoint.traffic = {azure_deployment_name: 100}\n",
        "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)  "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint florence-endpoint-ft exists\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": ".................................................................................................................................................................................................................\n---Deployment created successfully---\n\nCPU times: user 1.39 s, sys: 135 ms, total: 1.53 s\nWall time: 18min 3s\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Inference\n",
        "##### Test invocation\n",
        "Run inference on managed endpoint using sample data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os \n",
        "\n",
        "sample = {\n",
        "    \"input_data\": \n",
        "        [\n",
        "            {'role': 'system', 'content': {'You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- { \"name\": \"calculate_shipping_cost\", \"description\": \"Calculate the cost of shipping a package\", \"parameters\": { \"type\": \"object\", \"properties\": { \"weight\": { \"type\": \"number\", \"description\": \"The weight of the package in pounds\" }, \"destination\": { \"type\": \"string\", \"description\": \"The destination of the package\" } }, \"required\": [ \"weight\", \"destination\" ] }}}\"'}},\n",
        "            {'role': 'user', 'content': 'Can you help me with shipping cost for a package?'},\n",
        "            {'role': 'assistant', 'content': 'Sure! I can help you with that. Please provide me with the weight and destination of the package.'},\n",
        "            {'role': 'user', 'content': 'The weight of the package is 10 pounds and the destination is New York.'}\n",
        "        ],\n",
        "    \"params\": {\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_new_tokens\": 128,\n",
        "        \"do_sample\": True,\n",
        "        \"return_full_text\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "test_inference_dir = \"./inference\"\n",
        "os.makedirs(test_inference_dir, exist_ok=True)\n",
        "\n",
        "request_file = os.path.join(test_inference_dir, \"sample_request.json\")\n",
        "\n",
        "with open(request_file, \"w\") as f:\n",
        "    json.dump(sample, f)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028020321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=azure_endpoint_name,\n",
        "    deployment_name=d['serve']['azure_deployment_name'],\n",
        "    request_file=request_file\n",
        ")\n",
        "\n",
        "result_json = json.loads(result)\n",
        "result = result_json['result']\n",
        "\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'<DocVQA> ': '2'}\n"
        }
      ],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028024558
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Latency/Throughput Benchmarking"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from time import perf_counter\n",
        "\n",
        "def benchmark_latency(endpoint_name, deployment_name, sample_data_path, num_warmups=1, num_infers=5):\n",
        "    print(f\"Measuring latency for Endpoint '{endpoint_name}' and Deployment '{deployment_name}', num_infers={num_infers}\")\n",
        "\n",
        "    latencies = []\n",
        "    # warm up\n",
        "    for _ in range(num_warmups):\n",
        "        result = ml_client.online_endpoints.invoke(\n",
        "            endpoint_name=endpoint_name,\n",
        "            deployment_name=deployment_name,\n",
        "            request_file=sample_data_path,\n",
        "        ) \n",
        "        \n",
        "    begin = time.time()        \n",
        "    # Timed run\n",
        "    for _ in range(num_infers):\n",
        "        start_time = perf_counter()\n",
        "        result = ml_client.online_endpoints.invoke(\n",
        "            endpoint_name=endpoint_name,\n",
        "            deployment_name=deployment_name,\n",
        "            request_file=sample_data_path,\n",
        "        )\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "    end = time.time() \n",
        "        \n",
        "    # Compute run statistics\n",
        "    duration = end - begin    \n",
        "    time_avg_sec = np.mean(latencies)\n",
        "    time_std_sec = np.std(latencies)\n",
        "    time_p95_sec = np.percentile(latencies, 95)\n",
        "    time_p99_sec = np.percentile(latencies, 99)\n",
        "    \n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'duration': duration,\n",
        "        'avg_sec': time_avg_sec,\n",
        "        'std_sec': time_std_sec,        \n",
        "        'p95_sec': time_p95_sec,\n",
        "        'p99_sec': time_p99_sec    \n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def benchmark_latency_multicore(endpoint_name, deployment_name, sample_data_path, num_warmups=1, num_infers=5, num_threads=2):\n",
        "    import time\n",
        "    import concurrent.futures\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(num_warmups):\n",
        "        result = ml_client.online_endpoints.invoke(\n",
        "            endpoint_name=endpoint_name,\n",
        "            deployment_name=deployment_name,\n",
        "            request_file=sample_data_path,\n",
        "        )        \n",
        "                \n",
        "    latencies = []\n",
        "\n",
        "    # Thread task: Each of these thread tasks executes in a serial loop for a single model.\n",
        "    #              Multiple of these threads are launched to achieve parallelism.\n",
        "    def task(model):\n",
        "        for _ in range(num_infers):\n",
        "            start = time.time()\n",
        "            result = ml_client.online_endpoints.invoke(\n",
        "                endpoint_name=endpoint_name,\n",
        "                deployment_name=deployment_name,\n",
        "                request_file=sample_data_path,\n",
        "            )   \n",
        "            finish = time.time()\n",
        "            latencies.append(finish - start)\n",
        "            \n",
        "    # Submit tasks\n",
        "    begin = time.time()\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as pool:\n",
        "        for i in range(num_threads):\n",
        "            pool.submit(task, model)\n",
        "    end = time.time()\n",
        "\n",
        "    # Compute metrics\n",
        "    duration = end - begin\n",
        "    inferences = len(latencies)\n",
        "    throughput = inferences / duration\n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    \n",
        "    # Compute run statistics\n",
        "    time_avg_sec = np.mean(latencies)\n",
        "    time_std_sec = np.std(latencies)\n",
        "    time_p95_sec = np.percentile(latencies, 95)\n",
        "    time_p99_sec = np.percentile(latencies, 99)\n",
        "    \n",
        "    time_std_sec = np.std(latencies)\n",
        "    time_p95_sec = np.percentile(latencies, 95)\n",
        "    time_p99_sec = np.percentile(latencies, 99)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'threads': num_threads,\n",
        "        'duration': duration,\n",
        "        'throughput': throughput,\n",
        "        'avg_sec': avg_latency,\n",
        "        'std_sec': time_std_sec,        \n",
        "        'p95_sec': time_p95_sec,\n",
        "        'p99_sec': time_p99_sec    \n",
        "    }\n",
        "    \n",
        "    return metrics"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028107984
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_result = benchmark_latency(azure_endpoint_name, \"florence-vqa-ft-v1\", request_file, num_warmups=1, num_infers=10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Measuring latency for Endpoint 'florence-endpoint-ft' and Deployment 'florence-vqa-ft-v1', num_infers=10\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028171852
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(benchmark_result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'duration': 9.96614122390747, 'avg_sec': 0.9966129487998842, 'std_sec': 0.1501435052554758, 'p95_sec': 1.1999801447503158, 'p99_sec': 1.2242199113500283}\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028221400
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}