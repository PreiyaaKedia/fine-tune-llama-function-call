{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "import re \n",
        "\n",
        "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "with open('./llama-fc_config.yaml') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_SFT_DATA_NAME']    \n",
        "DATA_DIR = d['config']['SFT_DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
        "IS_DEBUG = d['config']['IS_DEBUG']\n",
        "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
        "\n",
        "azure_env_name = d['train']['azure_env_name']  \n",
        "azure_compute_cluster_name = d['train']['azure_compute_cluster_name']\n",
        "azure_compute_cluster_size = d['train']['azure_compute_cluster_size']\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CLOUD_DIR, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1735899799804
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create console handler with a higher log level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create file handler which logs even debug messages\n",
        "file_handler = logging.FileHandler(\"debug.log\")\n",
        "file_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(console_handler)\n",
        "logger.addHandler(file_handler)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735898757063
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
        "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
        "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
        "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
        "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
        "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
        "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
        "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
        "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
        "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
        "logger.info(f\"azure_env_name={azure_env_name}\")\n",
        "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
        "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-01-03 10:05:57,752 - __main__ - INFO - ===== 0. Azure ML Training Info =====\n2025-01-03 10:05:57,754 - __main__ - INFO - AZURE_SUBSCRIPTION_ID=8cebb108-a4d5-402b-a0c4-f7556126277f\n2025-01-03 10:05:57,755 - __main__ - INFO - AZURE_RESOURCE_GROUP=azure-ml-priya-demo\n2025-01-03 10:05:57,756 - __main__ - INFO - AZURE_WORKSPACE=azure-ml-priya-westus3\n2025-01-03 10:05:57,757 - __main__ - INFO - AZURE_DATA_NAME=sft-demo-data-function-call\n2025-01-03 10:05:57,757 - __main__ - INFO - DATA_DIR=./dataset\n2025-01-03 10:05:57,758 - __main__ - INFO - CLOUD_DIR=./cloud\n2025-01-03 10:05:57,759 - __main__ - INFO - HF_MODEL_NAME_OR_PATH=unsloth/Llama-3.2-3B-Instruct\n2025-01-03 10:05:57,760 - __main__ - INFO - IS_DEBUG=True\n2025-01-03 10:05:57,760 - __main__ - INFO - USE_LOWPRIORITY_VM=False\n2025-01-03 10:05:57,761 - __main__ - INFO - azure_env_name=slm-llama-acft-custom-env\n2025-01-03 10:05:57,762 - __main__ - INFO - azure_compute_cluster_name=gpu-a100-demo-vm\n2025-01-03 10:05:57,762 - __main__ - INFO - azure_compute_cluster_size=Standard_NC24ads_A100_v4\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735898758809
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Training Preparation\n",
        "#### 2.1 Configure Workspace Details\n",
        "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735898768893
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2. Load and prepare the dataset\n",
        "#####\n",
        "Training data can be used as a dataset stored in the local development environment, but can also be registered as AzureML data. For this hands-on session, we will register the data as AzureML Data asset and will use the registered dataset for training and inference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_data_version = max([int(d.version) for d in ml_client.data.list(name=data_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Data asset, but will update the Data.')            \n",
        "        else:\n",
        "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
        "            print(f\"Found Data asset: {data_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        data = Data(\n",
        "            path=data_local_dir,\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            description=f\"{data_name} for fine tuning\",\n",
        "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
        "            name=data_name\n",
        "        )\n",
        "        data_asset = ml_client.data.create_or_update(data)\n",
        "        print(f\"Created Data asset: {data_name}\")\n",
        "        \n",
        "    return data_asset"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735900366143
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train\")\n",
        "\n",
        "num_samples = len(dataset)\n",
        "\n",
        "train_dataset = dataset.select(range(2000))\n",
        "test_dataset = dataset.select(range(2000, 2200))\n",
        "val_dataset = dataset.select(range(2200, 2700))\n",
        "\n",
        "train_dataset.save_to_disk(f\"{DATA_DIR}/train\")\n",
        "val_dataset.save_to_disk(f\"{DATA_DIR}/val\")\n",
        "test_dataset.save_to_disk(f\"{DATA_DIR}/test\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Generating train split: 100%|██████████| 112960/112960 [00:01<00:00, 67240.95 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 2000/2000 [00:00<00:00, 26704.34 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 9809.40 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 5136.90 examples/s]\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735561197886
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_data_version = max([int(d.version) for d in ml_client.data.list(name=data_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Data asset, but will update the Data.')            \n",
        "        else:\n",
        "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
        "            logger.info(f\"Found Data asset: {data_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        data = Data(\n",
        "            path=data_local_dir,\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            description=f\"{data_name} for fine tuning\",\n",
        "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
        "            name=data_name\n",
        "        )\n",
        "        data_asset = ml_client.data.create_or_update(data)\n",
        "        logger.info(f\"Created/Updated Data asset: {data_name}\")\n",
        "        \n",
        "    return data_asset"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735900385777
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = get_or_create_data_asset(ml_client, f\"{AZURE_DATA_NAME}_train\", data_local_dir=f\"{DATA_DIR}/train\", update=True)\n",
        "val_data = get_or_create_data_asset(ml_client, f\"{AZURE_DATA_NAME}_val\", data_local_dir=f\"{DATA_DIR}/val\", update=True)\n",
        "test_data = get_or_create_data_asset(ml_client, f\"{AZURE_DATA_NAME}_test\", data_local_dir=f\"{DATA_DIR}/test\", update=True)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceNotFoundError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[41], line 4\u001b[0m, in \u001b[0;36mget_or_create_data_asset\u001b[0;34m(ml_client, data_name, data_local_dir, update)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     latest_data_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mint\u001b[39m(d\u001b[38;5;241m.\u001b[39mversion) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m ml_client\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mlist(name\u001b[38;5;241m=\u001b[39mdata_name)])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update:\n",
            "Cell \u001b[0;32mIn[41], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     latest_data_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m([\u001b[38;5;28mint\u001b[39m(d\u001b[38;5;241m.\u001b[39mversion) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m ml_client\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mlist(name\u001b[38;5;241m=\u001b[39mdata_name)])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/paging.py:123\u001b[0m, in \u001b[0;36mItemPaged.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby_page())\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_page_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/paging.py:75\u001b[0m, in \u001b[0;36mPageIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontinuation_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_data_versions_operations.py:353\u001b[0m, in \u001b[0;36mDataVersionsOperations.list.<locals>.get_next\u001b[0;34m(next_link)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m]:\n\u001b[0;32m--> 353\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/exceptions.py:161\u001b[0m, in \u001b[0;36mmap_error\u001b[0;34m(status_code, response, error_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
            "\u001b[0;31mResourceNotFoundError\u001b[0m: (UserError) sft-demo-data-function-call_train_v1 container was not found.\nCode: UserError\nMessage: sft-demo-data-function-call_train_v1 container was not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mget_or_create_data_asset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mAZURE_DATA_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_train_v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_local_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mDATA_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m val_data \u001b[38;5;241m=\u001b[39m get_or_create_data_asset(ml_client, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAZURE_DATA_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_val_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_local_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/val\u001b[39m\u001b[38;5;124m\"\u001b[39m, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test_data \u001b[38;5;241m=\u001b[39m get_or_create_data_asset(ml_client, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mAZURE_DATA_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_test_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_local_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test\u001b[39m\u001b[38;5;124m\"\u001b[39m, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[41], line 18\u001b[0m, in \u001b[0;36mget_or_create_data_asset\u001b[0;34m(ml_client, data_name, data_local_dir, update)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ResourceNotFoundError, ResourceExistsError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     data \u001b[38;5;241m=\u001b[39m Data(\n\u001b[1;32m     12\u001b[0m         path\u001b[38;5;241m=\u001b[39mdata_local_dir,\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mAssetTypes\u001b[38;5;241m.\u001b[39mURI_FOLDER,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         name\u001b[38;5;241m=\u001b[39mdata_name\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m     data_asset \u001b[38;5;241m=\u001b[39m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated/Updated Data asset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_asset\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_data_operations.py:367\u001b[0m, in \u001b[0;36mDataOperations.create_or_update\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m referenced_uris:\n\u001b[1;32m    365\u001b[0m     data\u001b[38;5;241m.\u001b[39m_referenced_uris \u001b[38;5;241m=\u001b[39m referenced_uris\n\u001b[0;32m--> 367\u001b[0m data, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_check_and_upload_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_operations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43msas_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msas_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mErrorTarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m _check_or_modify_auto_delete_setting(data\u001b[38;5;241m.\u001b[39mauto_delete_setting)\n\u001b[1;32m    377\u001b[0m data_version_resource \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_to_rest_object()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_artifacts/_artifact_utilities.py:506\u001b[0m, in \u001b[0;36m_check_and_upload_path\u001b[0;34m(artifact, asset_operations, artifact_type, datastore_name, sas_uri, show_progress, blob_uri)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_absolute():\n\u001b[1;32m    505\u001b[0m     path \u001b[38;5;241m=\u001b[39m Path(artifact\u001b[38;5;241m.\u001b[39mbase_path, path)\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[0;32m--> 506\u001b[0m uploaded_artifact \u001b[38;5;241m=\u001b[39m \u001b[43m_upload_to_datastore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_operations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_operation_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_operations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatastore_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatastore_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43martifact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43masset_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43martifact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_upload_hash\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43msas_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msas_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43martifact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_ignore_file\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblob_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblob_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m indicator_file \u001b[38;5;241m=\u001b[39m uploaded_artifact\u001b[38;5;241m.\u001b[39mindicator_file  \u001b[38;5;66;03m# reference to storage contents\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artifact\u001b[38;5;241m.\u001b[39m_is_anonymous:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_artifacts/_artifact_utilities.py:383\u001b[0m, in \u001b[0;36m_upload_to_datastore\u001b[0;34m(operation_scope, datastore_operation, path, artifact_type, datastore_name, show_progress, asset_name, asset_version, asset_hash, ignore_file, sas_uri, blob_uri)\u001b[0m\n\u001b[1;32m    381\u001b[0m     ignore_file \u001b[38;5;241m=\u001b[39m get_ignore_file(path)\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asset_hash:\n\u001b[0;32m--> 383\u001b[0m     asset_hash \u001b[38;5;241m=\u001b[39m \u001b[43mget_object_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m artifact \u001b[38;5;241m=\u001b[39m upload_artifact(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mstr\u001b[39m(path),\n\u001b[1;32m    386\u001b[0m     datastore_operation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m     sas_uri\u001b[38;5;241m=\u001b[39msas_uri,\n\u001b[1;32m    395\u001b[0m )\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blob_uri:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_asset_utils.py:298\u001b[0m, in \u001b[0;36mget_object_hash\u001b[0;34m(path, ignore_file)\u001b[0m\n\u001b[1;32m    296\u001b[0m _hash \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39mmd5(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialize for october 2021 AML CLI version\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# nosec\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 298\u001b[0m     object_hash \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dir_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_hash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mislink(path):  \u001b[38;5;66;03m# ensure we're hashing the contents of the linked file\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_asset_utils.py:260\u001b[0m, in \u001b[0;36m_get_dir_hash\u001b[0;34m(directory, _hash, ignore_file)\u001b[0m\n\u001b[1;32m    258\u001b[0m     path \u001b[38;5;241m=\u001b[39m _resolve_path(path)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[0;32m--> 260\u001b[0m     _hash \u001b[38;5;241m=\u001b[39m \u001b[43m_get_file_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m    262\u001b[0m     _hash \u001b[38;5;241m=\u001b[39m _get_dir_hash(path, _hash, ignore_file)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_utils/_asset_utils.py:245\u001b[0m, in \u001b[0;36m_get_file_hash\u001b[0;34m(filename, _hash)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_file_hash\u001b[39m(filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], _hash: hash_type) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m hash_type:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mstr\u001b[39m(filename), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    246\u001b[0m             _hash\u001b[38;5;241m.\u001b[39mupdate(chunk)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hash\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735900406109
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Create AzureML environment\n",
        "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). This hands-on uses conda yaml."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Conda Enviornment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/train/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.10\n",
        "  - pip=24.0\n",
        "  - pip:\n",
        "    - bitsandbytes==0.43.3\n",
        "    - transformers==4.44.2\n",
        "    - peft~=0.12\n",
        "    - accelerate~=0.33\n",
        "    - trl==0.10.1\n",
        "    - einops==0.8.0\n",
        "    - datasets==2.21.0\n",
        "    - wandb==0.17.8\n",
        "    - mlflow==2.16.0\n",
        "    - azureml-mlflow==1.57.0\n",
        "    - azureml-sdk==1.57.0\n",
        "    - torchvision==0.19.0\n",
        "    - torch==2.4.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/train/conda.yml\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Docker Enviornment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/train/Dockerfile\n",
        "\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
        "\n",
        "USER root\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update && apt-get -y upgrade\n",
        "RUN pip install --upgrade pip\n",
        "RUN apt-get install -y openssh-server openssh-client\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "\n",
        "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/train/Dockerfile\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "def get_or_create_environment_asset(ml_client, env_name, conda_yml=\"cloud/conda.yml\", update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")        \n",
        "        env_docker_image = Environment(\n",
        "            image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
        "            conda_file=conda_yml,\n",
        "            name=env_name,\n",
        "            description=\"Environment created for llm fine-tuning.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "        \n",
        "    return env_asset\n",
        "\n",
        "\n",
        "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        env_docker_image = Environment(\n",
        "            build=BuildContext(path=docker_dir),\n",
        "            name=env_name,\n",
        "            description=\"Environment created from a Docker context.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "    \n",
        "    return env_asset"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735557380248
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = get_or_create_docker_environment_asset(ml_client, azure_env_name, docker_dir=f\"{CLOUD_DIR}/train\", update=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found Environment asset: slm-llama-acft-custom-env. Will not create again\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735557382654
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Training\n",
        "#### 3.1 Create the compute cluster"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "### Create the compute cluster\n",
        "try:\n",
        "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
        "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
        "except Exception as ex:\n",
        "    print(\n",
        "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
        "    )\n",
        "    try:\n",
        "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
        "        tier = 'LowPriority' if USE_LOWPRIORITY_VM else 'Dedicated'\n",
        "        compute = AmlCompute(\n",
        "            name=azure_compute_cluster_name,\n",
        "            size=azure_compute_cluster_size,\n",
        "            tier=tier,\n",
        "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
        "        )\n",
        "        ml_client.compute.begin_create_or_update(compute).wait()\n",
        "    except Exception as e:\n",
        "        print(\"Error\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The compute cluster already exists! Reusing it for the current run\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735557410762
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.2 Start the training job\n",
        "\n",
        "The `command` allows user to configure the following key aspects.  \n",
        "  \n",
        "- **inputs** - This is the dictionary of inputs using name value pairs to the command.  \n",
        "  - **type** - The type of input. This can be a `uri_file` or `uri_folder`. The default is `uri_folder`.  \n",
        "  - **path** - The path to the file or folder. These can be local or remote files or folders. For remote files - http/https, wasb are supported.  \n",
        "    - Azure ML `data`/`dataset` or `datastore` are of type `uri_folder`. To use `data`/`dataset` as input, you can use registered dataset in the workspace using the format `'<data_name>:<version>'`. For example, `Input(type='uri_folder', path='my_dataset:1')`  \n",
        "  - **mode** - Mode of how the data should be delivered to the compute target. Allowed values are `ro_mount`, `rw_mount`, and `download`. Default is `ro_mount`.  \n",
        "  \n",
        "- **code** - This is the path where the code to run the command is located.  \n",
        "  \n",
        "- **compute** - The compute on which the command will run. You can run it on the local machine by using `local` for the compute.  \n",
        "  \n",
        "- **command** - This is the command that needs to be run using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, we can use the `Input` class. The `Input` class supports three parameters:  \n",
        "  \n",
        "- **environment** - This is the environment needed for the command to run. Curated (built-in) or custom environments from the workspace can be used.  \n",
        "  \n",
        "- **instance_count** - Number of nodes. Default is 1.  \n",
        "  \n",
        "- **distribution** - Distribution configuration for distributed training scenarios. Azure Machine Learning supports PyTorch, TensorFlow, and MPI-based distributed.  \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.entities import ResourceConfiguration\n",
        "\n",
        "job = command(\n",
        "    inputs=dict(\n",
        "        #train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
        "        train_dir=Input(path=f\"{AZURE_DATA_NAME}_train@latest\"),  # Get data from Data asset\n",
        "        val_dir = Input(path=f\"{AZURE_DATA_NAME}_val@latest\"),\n",
        "        epoch=d['train']['epoch'],\n",
        "        train_batch_size=d['train']['train_batch_size'],\n",
        "        eval_batch_size=d['train']['eval_batch_size'],  \n",
        "    ),\n",
        "    code=f\"{CLOUD_DIR}/train\",  # local path where the code is stored\n",
        "    compute=azure_compute_cluster_name,\n",
        "    command=\"python train_v3.py --train_dir ${{inputs.train_dir}} --val_dir ${{inputs.val_dir}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}}\",\n",
        "    #environment=\"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/77\", # Use built-in Environment asset\n",
        "    environment=f\"{azure_env_name}@latest\",\n",
        "    distribution={\n",
        "        \"type\": \"PyTorch\",\n",
        "        \"process_count_per_instance\": 1, # For multi-gpu training set this to an integer value more than 1\n",
        "    },\n",
        ")\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: tidy_gyro_bw85xpv3l6\nWeb View: https://ml.azure.com/runs/tidy_gyro_bw85xpv3l6?wsid=/subscriptions/8cebb108-a4d5-402b-a0c4-f7556126277f/resourcegroups/azure-ml-priya-demo/workspaces/azure-ml-priya-westus3\n\nExecution Summary\n=================\nRunId: tidy_gyro_bw85xpv3l6\nWeb View: https://ml.azure.com/runs/tidy_gyro_bw85xpv3l6?wsid=/subscriptions/8cebb108-a4d5-402b-a0c4-f7556126277f/resourcegroups/azure-ml-priya-demo/workspaces/azure-ml-priya-westus3\n\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735565684434
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Register the model for future deployment and inference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Run \n",
        "import os  \n",
        "  \n",
        "# Connect to your workspace  \n",
        "ws = Workspace.from_config()  \n",
        "  \n",
        "experiment_name =  'fine-tune-llama-32b-fc'\n",
        "run_id = 'tidy_gyro_bw85xpv3l6'\n",
        "\n",
        "run = Run(ws.experiments[experiment_name], run_id)  \n",
        "\n",
        "# Register the model  \n",
        "model = run.register_model(  \n",
        "    model_name=d[\"serve\"][\"azure_model_name\"],  # this is the name the model will be registered under  \n",
        "    model_path=\"outputs\"  # this is the path to the model file in the run's outputs  \n",
        ")  \n",
        "# Create a local directory to save the outputs  \n",
        "local_folder = './model'  \n",
        "os.makedirs(local_folder, exist_ok=True)  \n",
        "  \n",
        "# Download the entire outputs folder  \n",
        "run.download_files(prefix='outputs', output_directory=local_folder)  "
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735566088219
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}