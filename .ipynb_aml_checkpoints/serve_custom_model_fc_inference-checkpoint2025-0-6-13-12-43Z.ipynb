{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Serving Preparation\n",
        "##### \n",
        "##### 1.1 Configure Workspace details\n",
        "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "config_file = \"/llama-fc-wo-descriptions_config.yaml\"\n",
        "\n",
        "with open(f'./{config_file}') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_SFT_DATA_NAME']    \n",
        "DATA_DIR = d['config']['SFT_DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
        "IS_DEBUG = d['config']['IS_DEBUG']\n",
        "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
        "\n",
        "azure_env_name = d['serve']['azure_env_name']\n",
        "azure_compute_cluster_name = d['serve']['azure_compute_cluster_name']\n",
        "azure_compute_cluster_size = d['serve']['azure_serving_cluster_size']\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CLOUD_DIR, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1736166062564
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create console handler with a higher log level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create file handler which logs even debug messages\n",
        "file_handler = logging.FileHandler(\"debug.log\")\n",
        "file_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(console_handler)\n",
        "logger.addHandler(file_handler)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736166065613
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
        "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
        "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
        "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
        "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
        "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
        "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
        "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
        "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
        "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
        "logger.info(f\"azure_env_name={azure_env_name}\")\n",
        "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
        "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-01-06 12:21:06,802 - __main__ - INFO - ===== 0. Azure ML Training Info =====\n2025-01-06 12:21:06,803 - __main__ - INFO - AZURE_SUBSCRIPTION_ID=8cebb108-a4d5-402b-a0c4-f7556126277f\n2025-01-06 12:21:06,804 - __main__ - INFO - AZURE_RESOURCE_GROUP=azure-ml-priya-demo\n2025-01-06 12:21:06,807 - __main__ - INFO - AZURE_WORKSPACE=azure-ml-priya-westus3\n2025-01-06 12:21:06,808 - __main__ - INFO - AZURE_DATA_NAME=sft-data-function-call-wo-desc\n2025-01-06 12:21:06,809 - __main__ - INFO - DATA_DIR=./dataset_wo_desc\n2025-01-06 12:21:06,811 - __main__ - INFO - CLOUD_DIR=./cloud\n2025-01-06 12:21:06,812 - __main__ - INFO - HF_MODEL_NAME_OR_PATH=unsloth/Llama-3.2-3B-Instruct\n2025-01-06 12:21:06,813 - __main__ - INFO - IS_DEBUG=True\n2025-01-06 12:21:06,814 - __main__ - INFO - USE_LOWPRIORITY_VM=False\n2025-01-06 12:21:06,815 - __main__ - INFO - azure_env_name=slm-serving-llama\n2025-01-06 12:21:06,816 - __main__ - INFO - azure_compute_cluster_name=gpu-a100-demo-vm\n2025-01-06 12:21:06,817 - __main__ - INFO - azure_compute_cluster_size=Standard_NC24ads_A100_v4\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736166067381
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736166078573
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1.2 Create Model asset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Run\n",
        "\n",
        "def get_or_create_model_asset(ml_client, model_name, experiment_name = None, job_name = None, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
        "        else:\n",
        "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
        "            print(f\"Found Model asset: {model_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\") \n",
        "        ws = Workspace.from_config()  \n",
        "  \n",
        "        # Get the run by its ID   \n",
        "        run = Run(ws.experiments[experiment_name], job_name)  \n",
        "        # Register the model  \n",
        "        model_asset = run.register_model(  \n",
        "            model_name=model_name,  # this is the name the model will be registered under  \n",
        "            model_path=model_dir  # this is the path to the model file in the run's outputs  \n",
        "        )         \n",
        "        print(f\"Created Model asset: {model_name}\")\n",
        "\n",
        "    return model_asset"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735288514090
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_or_create_model_asset(ml_client, model_name, job_name, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
        "    \n",
        "    try:\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
        "        else:\n",
        "            latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
        "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
        "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        logger.info(f\"Exception: {e}\")        \n",
        "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"    \n",
        "        run_model = Model(\n",
        "            name=model_name,        \n",
        "            path=model_path,\n",
        "            description=\"Model created from run.\",\n",
        "            type=model_type # mlflow_model, custom_model, triton_model\n",
        "        )\n",
        "        model_asset = ml_client.models.create_or_update(run_model)\n",
        "        logger.info(f\"Created Model asset: {model_name}\")\n",
        "\n",
        "    return model_asset"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736157610393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = d['train']['model_dir']\n",
        "model = get_or_create_model_asset(ml_client, d[\"serve\"][\"azure_model_name\"], \"quirky_plastic_hsm6nbb4rk\", model_dir, model_type=\"custom_model\", update=False)\n",
        "\n",
        "# model = get_or_create_model_asset(ml_client, d['serve']['azure_model_name'], update = False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2025-01-06 10:00:35,022 - __main__ - INFO - Found Model asset: llama-fc-wo-descriptions-ft. Will not create again\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736157635074
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Create AzureML environment\n",
        "####\n",
        "Azure ML defines containers (called environment asset) in which your code will run. You can use a pre-built enviornment or create a custom enviornment. For this hands-on session, we will buid a custom Docker enviornment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/serve/Dockerfile\n",
        "\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "\n",
        "# Inference requirements\n",
        "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
        "\n",
        "RUN /var/requirements/install_system_requirements.sh && \\\n",
        "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
        "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
        "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
        "    rm -f /etc/nginx/sites-enabled/default\n",
        "ENV SVDIR=/var/runit\n",
        "ENV WORKER_TIMEOUT=400\n",
        "EXPOSE 5001 8883 8888\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y openssh-server openssh-client\n",
        "\n",
        "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/serve/Dockerfile\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/serve/requirements.txt\n",
        "azureml-core==1.58.0\n",
        "azureml-dataset-runtime==1.58.0\n",
        "azureml-defaults==1.58.0\n",
        "azure-ml==0.0.1\n",
        "azure-ml-component==0.9.18.post2\n",
        "azureml-mlflow==1.58.0\n",
        "azureml-contrib-services==1.58.0\n",
        "azureml-contrib-services==1.58.0\n",
        "azureml-automl-common-tools==1.58.0\n",
        "torch-tb-profiler==0.4.3\n",
        "azureml-inference-server-http~=1.3\n",
        "inference-schema==1.8.0\n",
        "MarkupSafe==3.0.2\n",
        "regex\n",
        "pybind11\n",
        "bitsandbytes==0.44.1\n",
        "transformers==4.46.1\n",
        "peft==0.13.2\n",
        "accelerate==1.1.0\n",
        "datasets\n",
        "scipy\n",
        "azure-identity\n",
        "packaging==24.1\n",
        "timm==1.0.11\n",
        "einops"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/serve/requirements.txt\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        env_docker_image = Environment(\n",
        "            build=BuildContext(path=docker_dir),\n",
        "            name=env_name,\n",
        "            description=\"Environment created from a Docker context.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "    \n",
        "    return env_asset\n",
        "\n",
        "env = get_or_create_docker_environment_asset(ml_client, \"slm-serving-florence\", f\"{CLOUD_DIR}/inference\", update=False)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found Environment asset: slm-serving-florence. Will not create again\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736157687293
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4 Serving script\n",
        "####\n",
        "If you are not serving using a MLflow model but instead using a custom model, you can write your own script. This step demosntrates how to write the scoring script to run the inference.\n",
        "\n",
        "The scoring script consists of two components:\n",
        "\n",
        "1. init() : This is where you define the global initialization logic like loading of LLM models and tokenizers\n",
        "2. run() : Inference logic called for every invocation of the endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/inference/score.py\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import base64\n",
        "import logging\n",
        "\n",
        "from io import BytesIO\n",
        "from transformers import AutoTokenizer, AutoProcessor, pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    global tokenizer\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # Please provide your model's folder name if there is one\n",
        "    model_name_or_path = os.path.join(\n",
        "        os.getenv(\"AZUREML_MODEL_DIR\"), \"{{score_model_dir}}\"\n",
        "    )\n",
        "    \n",
        "    model_kwargs = dict(\n",
        "        trust_remote_code=True,    \n",
        "        device_map={\"\":0},\n",
        "        torch_dtype=\"auto\" \n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)    \n",
        "\n",
        "    logging.info(\"Loaded model.\")\n",
        "    \n",
        "def run(json_data: str):\n",
        "    logging.info(\"Request received\")\n",
        "    data = json.loads(json_data)\n",
        "    input_data = data[\"input_data\"]\n",
        "    params = data['params']\n",
        "\n",
        "    # pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
        "    # output = pipe(input_data, **params)\n",
        "    # result = output[0][\"generated_text\"]\n",
        "    # logging.info(f\"Generated text : {result}\")\n",
        "    inputs = tokenizer.apply_chat_template(input_data, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, do_sample = True, temperature = 0.1)\n",
        "    result = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens = True)\n",
        "\n",
        "    json_result = {\"result\" : result}\n",
        "\n",
        "    return json_result"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./cloud/inference/score.py\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plug in the appropriate variable in the inference script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jinja2\n",
        "from pathlib import Path\n",
        "TRAINED_MLFLOW = False\n",
        "\n",
        "jinja_env = jinja2.Environment()  \n",
        "\n",
        "template = jinja_env.from_string(Path(f\"{CLOUD_DIR}/inference/score.py\").open().read())\n",
        "score_model_dir = model_dir.split(\"/\")[-1]\n",
        "\n",
        "Path(f\"{CLOUD_DIR}/inference/score.py\").open(\"w\").write(\n",
        "    template.render(score_model_dir=score_model_dir)\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "1986"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736163391659
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Serving\n",
        "#####\n",
        "##### 2.1 Create the endpoint\n",
        "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model.\n",
        " \n",
        "Note : This step doesn't provision the GPU"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    IdentityConfiguration,\n",
        "    ManagedIdentityConfiguration,\n",
        ")\n",
        "\n",
        "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
        "# Check if the endpoint already exists in the workspace\n",
        "try:\n",
        "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
        "    print(\"---Endpoint already exists---\")\n",
        "except:\n",
        "    # Create an online endpoint if it doesn't exist\n",
        "\n",
        "    # Define the endpoint\n",
        "    endpoint = ManagedOnlineEndpoint(\n",
        "        name=azure_endpoint_name,\n",
        "        description=f\"Test endpoint for {model.name}\",\n",
        "        # identity=IdentityConfiguration(\n",
        "        #     type=\"user_assigned\",\n",
        "        #     user_assigned_identities=[ManagedIdentityConfiguration(resource_id=uai_id)],\n",
        "        # )\n",
        "        # if uai_id != \"\"\n",
        "        # else None,\n",
        "    )\n",
        "\n",
        "# Trigger the endpoint creation\n",
        "try:\n",
        "    ml_client.begin_create_or_update(endpoint).wait()\n",
        "    print(\"\\n---Endpoint created successfully---\\n\")\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Endpoint creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n---Endpoint created successfully---\n\nCPU times: user 67.9 ms, sys: 4.82 ms, total: 72.7 ms\nWall time: 1min 4s\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2 Create the Deployment\n",
        "This process takes lot of time as GPU clusters needs to be provisioned and serving environment must be built"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from azure.ai.ml.entities import (    \n",
        "    OnlineRequestSettings,\n",
        "    CodeConfiguration,\n",
        "    ManagedOnlineDeployment,\n",
        "    ProbeSettings,\n",
        "    Environment\n",
        ")\n",
        "\n",
        "azure_deployment_name = f\"{d['serve']['azure_deployment_name']}\"\n",
        "\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=azure_deployment_name,\n",
        "    endpoint_name=azure_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=azure_compute_cluster_size,\n",
        "    instance_count=1,\n",
        "    #code_configuration=code_configuration,\n",
        "    environment = env,\n",
        "    scoring_script=\"score.py\",\n",
        "    code_path=f\"./{CLOUD_DIR}/inference\",\n",
        "    #environment_variables=deployment_env_vars,\n",
        "    request_settings=OnlineRequestSettings(max_concurrent_requests_per_instance=50,\n",
        "                                           request_timeout_ms=90000, max_queue_wait_ms=60000),\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        period=100,\n",
        "        initial_delay=500,\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        period=100,\n",
        "        initial_delay=500,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Trigger the deployment creation\n",
        "try:\n",
        "    ml_client.begin_create_or_update(deployment).wait()\n",
        "    print(\"\\n---Deployment created successfully---\\n\")\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err\n",
        "    \n",
        "endpoint.traffic = {azure_deployment_name: 100}\n",
        "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)  "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint llama-endpoint-ft exists\n\u001b[32mUploading inference (0.0 MBs): 100%|██████████| 2301/2301 [00:00<00:00, 12618.12it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "......................................................................................................."
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Inference\n",
        "##### Test invocation\n",
        "Run inference on managed endpoint using sample data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os \n",
        "\n",
        "sample = {\n",
        "    \"input_data\": \n",
        "        [\n",
        "            {'role': 'system', 'content': 'You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- { \"name\": \"calculate_shipping_cost\", \"description\": \"Calculate the cost of shipping a package\", \"parameters\": { \"type\": \"object\", \"properties\": { \"weight\": { \"type\": \"number\", \"description\": \"The weight of the package in pounds\" }, \"destination\": { \"type\": \"string\", \"description\": \"The destination of the package\" } }, \"required\": [ \"weight\", \"destination\" ] }}}\"'},\n",
        "            {'role': 'user', 'content': 'Can you help me with shipping cost for a package?'},\n",
        "            {'role': 'assistant', 'content': 'Sure! I can help you with that. Please provide me with the weight and destination of the package.'},\n",
        "            {'role': 'user', 'content': 'The weight of the package is 10 pounds and the destination is New York.'}\n",
        "        ],\n",
        "    \"params\": {\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"do_sample\": True,\n",
        "        \"return_full_text\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "# sample = {\n",
        "#     \"input_data\": \n",
        "#         [\n",
        "#             {\"role\": \"user\", \"content\": \"Tell me Microsoft's brief history.\"},\n",
        "#             {\"role\": \"assistant\", \"content\": \"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell a BASIC interpreter for the Altair 8800.\"},\n",
        "#             {\"role\": \"user\", \"content\": \"What about Amazon's history?\"}\n",
        "#         ],\n",
        "#     \"params\": {\n",
        "#         \"temperature\": 0.1,\n",
        "#         \"max_new_tokens\": 128,\n",
        "#         \"do_sample\": True,\n",
        "#         \"return_full_text\": False\n",
        "#     }\n",
        "# }\n",
        "\n",
        "test_inference_dir = \"./inference\"\n",
        "os.makedirs(test_inference_dir, exist_ok=True)\n",
        "\n",
        "request_file = os.path.join(test_inference_dir, \"sample_request.json\")\n",
        "\n",
        "with open(request_file, \"w\") as f:\n",
        "    json.dump(sample, f)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736166092725
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "import os\n",
        "import ssl\n",
        "\n",
        "# Request data goes here\n",
        "# The example below assumes JSON formatting which may be updated\n",
        "# depending on the format your endpoint expects.\n",
        "# More information can be found here:\n",
        "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "data = {}\n",
        "\n",
        "body = str.encode(json.dumps(data))\n",
        "\n",
        "url = 'https://llama-endpoint-ft.westus3.inference.ml.azure.com/score'\n",
        "# Replace this with the primary/secondary key, AMLToken, or Microsoft Entra ID token for the endpoint\n",
        "api_key = ''\n",
        "if not api_key:\n",
        "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
        "\n",
        "\n",
        "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
        "\n",
        "req = urllib.request.Request(url, body, headers)\n",
        "\n",
        "try:\n",
        "    response = urllib.request.urlopen(req)\n",
        "\n",
        "    result = response.read()\n",
        "    print(result)\n",
        "except urllib.error.HTTPError as error:\n",
        "    print(\"The request failed with status code: \" + str(error.code))\n",
        "\n",
        "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
        "    print(error.info())\n",
        "    print(error.read().decode(\"utf8\", 'ignore'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
        "azure_deployment_name = f\"{d['serve']['azure_deployment_name']}\"\n",
        "\n",
        "result = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=azure_endpoint_name,\n",
        "    deployment_name=azure_deployment_name,\n",
        "    request_file=request_file\n",
        ")\n",
        "\n",
        "result_json = json.loads(result)\n",
        "result = result_json['result']\n",
        "\n",
        "print(result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'tool_uses': [{'recipient_name': 'functions.calculate_shipping_cost', 'parameters': {'weight': 10, 'destination': 'New York'}}]}\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736166137490
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Latency/Throughput Benchmarking"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from time import perf_counter\n",
        "\n",
        "def benchmark_latency(endpoint_name, deployment_name, sample_data_path, num_warmups=1, num_infers=5):\n",
        "    print(f\"Measuring latency for Endpoint '{endpoint_name}' and Deployment '{deployment_name}', num_infers={num_infers}\")\n",
        "\n",
        "    latencies = []\n",
        "    # warm up\n",
        "    for _ in range(num_warmups):\n",
        "        result = ml_client.online_endpoints.invoke(\n",
        "            endpoint_name=endpoint_name,\n",
        "            deployment_name=deployment_name,\n",
        "            request_file=sample_data_path,\n",
        "        ) \n",
        "        \n",
        "    begin = time.time()        \n",
        "    # Timed run\n",
        "    for _ in range(num_infers):\n",
        "        start_time = perf_counter()\n",
        "        result = ml_client.online_endpoints.invoke(\n",
        "            endpoint_name=endpoint_name,\n",
        "            deployment_name=deployment_name,\n",
        "            request_file=sample_data_path,\n",
        "        )\n",
        "        latency = perf_counter() - start_time\n",
        "        latencies.append(latency)\n",
        "    end = time.time() \n",
        "        \n",
        "    # Compute run statistics\n",
        "    duration = end - begin    \n",
        "    time_avg_sec = np.mean(latencies)\n",
        "    time_std_sec = np.std(latencies)\n",
        "    time_p95_sec = np.percentile(latencies, 95)\n",
        "    time_p99_sec = np.percentile(latencies, 99)\n",
        "    \n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'duration': duration,\n",
        "        'avg_sec': time_avg_sec,\n",
        "        'std_sec': time_std_sec,        \n",
        "        'p95_sec': time_p95_sec,\n",
        "        'p99_sec': time_p99_sec    \n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "def benchmark_latency_multicore(endpoint_name, deployment_name, sample_data_path, num_warmups=1, num_infers=5, num_threads=2):\n",
        "    import time\n",
        "    import concurrent.futures\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(num_warmups):\n",
        "        result = ml_client.online_endpoints.invoke(\n",
        "            endpoint_name=endpoint_name,\n",
        "            deployment_name=deployment_name,\n",
        "            request_file=sample_data_path,\n",
        "        )        \n",
        "                \n",
        "    latencies = []\n",
        "\n",
        "    # Thread task: Each of these thread tasks executes in a serial loop for a single model.\n",
        "    #              Multiple of these threads are launched to achieve parallelism.\n",
        "    def task(model):\n",
        "        for _ in range(num_infers):\n",
        "            start = time.time()\n",
        "            result = ml_client.online_endpoints.invoke(\n",
        "                endpoint_name=endpoint_name,\n",
        "                deployment_name=deployment_name,\n",
        "                request_file=sample_data_path,\n",
        "            )   \n",
        "            finish = time.time()\n",
        "            latencies.append(finish - start)\n",
        "            \n",
        "    # Submit tasks\n",
        "    begin = time.time()\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as pool:\n",
        "        for i in range(num_threads):\n",
        "            pool.submit(task, model)\n",
        "    end = time.time()\n",
        "\n",
        "    # Compute metrics\n",
        "    duration = end - begin\n",
        "    inferences = len(latencies)\n",
        "    throughput = inferences / duration\n",
        "    avg_latency = sum(latencies) / len(latencies)\n",
        "    \n",
        "    # Compute run statistics\n",
        "    time_avg_sec = np.mean(latencies)\n",
        "    time_std_sec = np.std(latencies)\n",
        "    time_p95_sec = np.percentile(latencies, 95)\n",
        "    time_p99_sec = np.percentile(latencies, 99)\n",
        "    \n",
        "    time_std_sec = np.std(latencies)\n",
        "    time_p95_sec = np.percentile(latencies, 95)\n",
        "    time_p99_sec = np.percentile(latencies, 99)\n",
        "\n",
        "    # Metrics\n",
        "    metrics = {\n",
        "        'threads': num_threads,\n",
        "        'duration': duration,\n",
        "        'throughput': throughput,\n",
        "        'avg_sec': avg_latency,\n",
        "        'std_sec': time_std_sec,        \n",
        "        'p95_sec': time_p95_sec,\n",
        "        'p99_sec': time_p99_sec    \n",
        "    }\n",
        "    \n",
        "    return metrics"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028107984
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_result = benchmark_latency(azure_endpoint_name, \"florence-vqa-ft-v1\", request_file, num_warmups=1, num_infers=10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Measuring latency for Endpoint 'florence-endpoint-ft' and Deployment 'florence-vqa-ft-v1', num_infers=10\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028171852
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(benchmark_result)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'duration': 9.96614122390747, 'avg_sec': 0.9966129487998842, 'std_sec': 0.1501435052554758, 'p95_sec': 1.1999801447503158, 'p99_sec': 1.2242199113500283}\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735028221400
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}