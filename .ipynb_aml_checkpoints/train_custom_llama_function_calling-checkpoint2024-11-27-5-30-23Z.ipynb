{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "with open('./llama-fc_config.yaml') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_SFT_DATA_NAME']    \n",
        "DATA_DIR = d['config']['SFT_DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
        "IS_DEBUG = d['config']['IS_DEBUG']\n",
        "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
        "\n",
        "azure_env_name = d['train']['azure_env_name']  \n",
        "azure_compute_cluster_name = d['train']['azure_compute_cluster_name']\n",
        "azure_compute_cluster_size = d['train']['azure_compute_cluster_size']\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CLOUD_DIR, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1735203226856
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create console handler with a higher log level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create file handler which logs even debug messages\n",
        "file_handler = logging.FileHandler(\"debug.log\")\n",
        "file_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(console_handler)\n",
        "logger.addHandler(file_handler)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735203235859
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
        "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
        "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
        "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
        "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
        "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
        "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
        "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
        "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
        "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
        "logger.info(f\"azure_env_name={azure_env_name}\")\n",
        "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
        "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2024-12-26 08:54:05,398 - __main__ - INFO - ===== 0. Azure ML Training Info =====\n2024-12-26 08:54:05,399 - __main__ - INFO - AZURE_SUBSCRIPTION_ID=8cebb108-a4d5-402b-a0c4-f7556126277f\n2024-12-26 08:54:05,399 - __main__ - INFO - AZURE_RESOURCE_GROUP=azure-ml-priya-demo\n2024-12-26 08:54:05,400 - __main__ - INFO - AZURE_WORKSPACE=azure-ml-priya-westus3\n2024-12-26 08:54:05,400 - __main__ - INFO - AZURE_DATA_NAME=sft-demo-data-function-call\n2024-12-26 08:54:05,401 - __main__ - INFO - DATA_DIR=./dataset\n2024-12-26 08:54:05,401 - __main__ - INFO - CLOUD_DIR=./cloud\n2024-12-26 08:54:05,402 - __main__ - INFO - HF_MODEL_NAME_OR_PATH=unsloth/Llama-3.2-3B-Instruct\n2024-12-26 08:54:05,403 - __main__ - INFO - IS_DEBUG=True\n2024-12-26 08:54:05,404 - __main__ - INFO - USE_LOWPRIORITY_VM=False\n2024-12-26 08:54:05,404 - __main__ - INFO - azure_env_name=slm-llama-acft-custom-env\n2024-12-26 08:54:05,404 - __main__ - INFO - azure_compute_cluster_name=gpu-a100-demo-vm\n2024-12-26 08:54:05,409 - __main__ - INFO - azure_compute_cluster_size=Standard_NC24ads_A100_v4\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735203244909
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Training Preparation\n",
        "#### 2.1 Configure Workspace Details\n",
        "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735204427487
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2. Load and prepare the dataset\n",
        "#####\n",
        "Training data can be used as a dataset stored in the local development environment, but can also be registered as AzureML data. For this hands-on session, we will register the data as AzureML Data asset and will use the registered dataset for training and inference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, load_from_disk\n",
        "\n",
        "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_data_version = max([int(d.version) for d in ml_client.data.list(name=data_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Data asset, but will update the Data.')            \n",
        "        else:\n",
        "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
        "            print(f\"Found Data asset: {data_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        data = Data(\n",
        "            path=data_local_dir,\n",
        "            type=AssetTypes.URI_FOLDER,\n",
        "            description=f\"{data_name} for fine tuning\",\n",
        "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
        "            name=data_name\n",
        "        )\n",
        "        data_asset = ml_client.data.create_or_update(data)\n",
        "        print(f\"Created Data asset: {data_name}\")\n",
        "        \n",
        "    return data_asset"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735204231230
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"glaiveai/glaive-function-calling\", split=\"train\")\n",
        "\n",
        "num_samples = len(dataset)\n",
        "\n",
        "train_dataset = dataset.select(range(1000))\n",
        "test_dataset = dataset.select(range(1000, 1500))\n",
        "val_dataset = dataset.select(range(1500, 2000))\n",
        "\n",
        "# train_dataset.save_to_disk(f\"{DATA_DIR}/train\")\n",
        "# val_dataset.save_to_disk(f\"{DATA_DIR}/val\")\n",
        "# test_dataset.save_to_disk(f\"{DATA_DIR}/test\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 17017.71 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 9361.20 examples/s]\nSaving the dataset (1/1 shards): 100%|██████████| 500/500 [00:00<00:00, 10160.67 examples/s]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735204240882
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735220342953
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!{sys.executable} -m pip install unsloth"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting unsloth\n  Using cached unsloth-2024.12.11-py3-none-any.whl.metadata (59 kB)\nCollecting unsloth_zoo>=2024.12.5 (from unsloth)\n  Using cached unsloth_zoo-2024.12.6-py3-none-any.whl.metadata (16 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Using cached xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nCollecting bitsandbytes (from unsloth)\n  Using cached bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: packaging in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (24.0)\nCollecting tyro (from unsloth)\n  Using cached tyro-0.9.5-py3-none-any.whl.metadata (9.4 kB)\nCollecting transformers!=4.47.0,>=4.46.1 (from unsloth)\n  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: datasets>=2.16.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (3.2.0)\nCollecting sentencepiece>=0.2.0 (from unsloth)\n  Using cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (4.66.4)\nRequirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (5.9.8)\nRequirement already satisfied: wheel>=0.42.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (0.43.0)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (1.23.5)\nCollecting accelerate>=0.34.1 (from unsloth)\n  Using cached accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\nCollecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n  Using cached trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nCollecting peft!=0.11.0,>=0.7.1 (from unsloth)\n  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nCollecting protobuf<4.0.0 (from unsloth)\n  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\nRequirement already satisfied: huggingface_hub in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth) (0.24.6)\nCollecting hf_transfer (from unsloth)\n  Using cached hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: pyyaml in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth) (6.0.1)\nRequirement already satisfied: safetensors>=0.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate>=0.34.1->unsloth) (0.4.4)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.14.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (15.0.2)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (0.3.6)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\nRequirement already satisfied: xxhash in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (0.70.14)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2023.10.0)\nRequirement already satisfied: aiohttp in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth) (3.10.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface_hub->unsloth) (4.12.2)\nCollecting huggingface_hub (from unsloth)\n  Using cached huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.3)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.4.0->unsloth) (3.1.4)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting sympy==1.13.1 (from torch>=2.4.0->unsloth)\n  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.7.24)\nCollecting tokenizers<0.22,>=0.21 (from transformers!=4.47.0,>=4.46.1->unsloth)\n  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting rich (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth)\n  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nCollecting cut_cross_entropy (from unsloth_zoo>=2024.12.5->unsloth)\n  Using cached cut_cross_entropy-24.12.3-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from unsloth_zoo>=2024.12.5->unsloth) (10.3.0)\nCollecting docstring-parser>=0.15 (from tyro->unsloth)\n  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nCollecting shtab>=1.5.6 (from tyro->unsloth)\n  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typeguard>=4.0.0 (from tyro->unsloth)\n  Using cached typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.1)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.6.2)\nCollecting markdown-it-py>=2.2.0 (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth)\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth) (2.18.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\nRequirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth) (2024.1)\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth)\n  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\nUsing cached unsloth-2024.12.11-py3-none-any.whl (175 kB)\nUsing cached accelerate-1.2.1-py3-none-any.whl (336 kB)\nUsing cached peft-0.14.0-py3-none-any.whl (374 kB)\nUsing cached huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\nUsing cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\nUsing cached sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nUsing cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\nUsing cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\nUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\nUsing cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\nUsing cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\nUsing cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\nUsing cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\nDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2024.12.6-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.28.post3-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.9.5-py3-none-any.whl (112 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typeguard-4.4.1-py3-none-any.whl (35 kB)\nDownloading cut_cross_entropy-24.12.3-py3-none-any.whl (22 kB)\nDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nInstalling collected packages: sentencepiece, typeguard, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mdurl, hf_transfer, docstring-parser, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, huggingface_hub, tokenizers, rich, nvidia-cusolver-cu12, tyro, transformers, torch, xformers, cut_cross_entropy, bitsandbytes, accelerate, trl, peft, unsloth_zoo, unsloth\n  Attempting uninstall: triton\n    Found existing installation: triton 2.3.0\n    Uninstalling triton-2.3.0:\n      Successfully uninstalled triton-2.3.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.3\n    Uninstalling sympy-1.13.3:\n      Successfully uninstalled sympy-1.13.3\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.3\n    Uninstalling protobuf-4.25.3:\n      Successfully uninstalled protobuf-4.25.3\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.1.105\n    Uninstalling nvidia-nvtx-cu12-12.1.105:\n      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.68\n    Uninstalling nvidia-nvjitlink-cu12-12.6.68:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.68\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.20.5\n    Uninstalling nvidia-nccl-cu12-2.20.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.2.106\n    Uninstalling nvidia-curand-cu12-10.3.2.106:\n      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.24.6\n    Uninstalling huggingface-hub-0.24.6:\n      Successfully uninstalled huggingface-hub-0.24.6\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.44.2\n    Uninstalling transformers-4.44.2:\n      Successfully uninstalled transformers-4.44.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.3.0\n    Uninstalling torch-2.3.0:\n      Successfully uninstalled torch-2.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.18.0 requires torch==2.3.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.2.1 bitsandbytes-0.45.0 cut_cross_entropy-24.12.3 docstring-parser-0.16 hf_transfer-0.1.8 huggingface_hub-0.27.0 markdown-it-py-3.0.0 mdurl-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 peft-0.14.0 protobuf-3.20.3 rich-13.9.4 sentencepiece-0.2.0 shtab-1.7.1 sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 transformers-4.47.1 triton-3.1.0 trl-0.13.0 typeguard-4.4.1 tyro-0.9.5 unsloth-2024.12.11 unsloth_zoo-2024.12.6 xformers-0.0.28.post3\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "def prepare_dataset(tokenizer):\n",
        "\n",
        "    # Map the strings/indetifiers from HF dataset to chatml template format\n",
        "    tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"USER\", \"assistant\": \"ASSISTANT\", \"system\": \"SYSTEM\"},\n",
        "    chat_template=\"chatml\",\n",
        "    )\n",
        "\n",
        "    # Load the dataset from disk\n",
        "    train_dataset = load_from_disk(f\"{DATA_DIR}_train\") \n",
        "    eval_dataset = load_from_disk(f\"{DATA_DIR}_val\")\n",
        "\n",
        "    column_names = list(train_dataset.features)\n",
        "\n",
        "    def apply_chat_template(examples):\n",
        "        messages = examples[\"sample\"]\n",
        "        conversations = []\n",
        "        for message in messages:\n",
        "            conversations.append(parse_conversation(message))\n",
        "        text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in conversations]\n",
        "\n",
        "        return {\"text\": text}\n",
        "\n",
        "\n",
        "    # process the dataseta and drop unused columns\n",
        "    train_dataset = train_dataset.map(apply_chat_template, batched = True, remove_columns=column_names)\n",
        "    eval_dataset = eval_dataset.map(apply_chat_template, batched = True, remove_columns=column_names)\n",
        "\n",
        "    return train_dataset, eval_dataset\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'unsloth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_templates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_chat_template\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset\u001b[39m(tokenizer):\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Map the strings/indetifiers from HF dataset to chatml template format\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m get_chat_template(\n\u001b[1;32m      7\u001b[0m     tokenizer,\n\u001b[1;32m      8\u001b[0m     mapping\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASSISTANT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSYSTEM\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      9\u001b[0m     chat_template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     )\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unsloth'"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735220327567
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = get_or_create_data_asset(ml_client, f\"{AZURE_DATA_NAME}_train\", data_local_dir=f\"{DATA_DIR}/train\", update=True)\n",
        "val_data = get_or_create_data_asset(ml_client, f\"{AZURE_DATA_NAME}_val\", data_local_dir=f\"{DATA_DIR}/val\", update=True)\n",
        "test_data = get_or_create_data_asset(ml_client, f\"{AZURE_DATA_NAME}_test\", data_local_dir=f\"{DATA_DIR}/test\", update=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading train (2.0 MBs): 100%|██████████| 2003913/2003913 [00:00<00:00, 19557019.60it/s]\n\u001b[39m\n\n\u001b[32mUploading val (1.03 MBs): 100%|██████████| 1031422/1031422 [00:00<00:00, 19908684.52it/s]\n\u001b[39m\n\n\u001b[32mUploading test (1.01 MBs): 100%|██████████| 1006590/1006590 [00:00<00:00, 32490741.81it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created Data asset: sft-demo-data-function-call_train\nCreated Data asset: sft-demo-data-function-call_val\nCreated Data asset: sft-demo-data-function-call_test\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735204361843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Create AzureML environment\n",
        "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). This hands-on uses conda yaml."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Conda Enviornment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/train/conda.yml\n",
        "name: model-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.10\n",
        "  - pip=24.0\n",
        "  - pip:\n",
        "    - bitsandbytes==0.43.3\n",
        "    - transformers==4.44.2\n",
        "    - peft~=0.12\n",
        "    - accelerate~=0.33\n",
        "    - trl==0.10.1\n",
        "    - einops==0.8.0\n",
        "    - datasets==2.21.0\n",
        "    - wandb==0.17.8\n",
        "    - mlflow==2.16.0\n",
        "    - azureml-mlflow==1.57.0\n",
        "    - azureml-sdk==1.57.0\n",
        "    - torchvision==0.19.0\n",
        "    - torch==2.4.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/train/conda.yml\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Docker Enviornment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {CLOUD_DIR}/train/Dockerfile\n",
        "\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
        "\n",
        "USER root\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update && apt-get -y upgrade\n",
        "RUN pip install --upgrade pip\n",
        "RUN apt-get install -y openssh-server openssh-client\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "\n",
        "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./cloud/train/Dockerfile\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "def get_or_create_environment_asset(ml_client, env_name, conda_yml=\"cloud/conda.yml\", update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")        \n",
        "        env_docker_image = Environment(\n",
        "            image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
        "            conda_file=conda_yml,\n",
        "            name=env_name,\n",
        "            description=\"Environment created for llm fine-tuning.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "        \n",
        "    return env_asset\n",
        "\n",
        "\n",
        "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        env_docker_image = Environment(\n",
        "            build=BuildContext(path=docker_dir),\n",
        "            name=env_name,\n",
        "            description=\"Environment created from a Docker context.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "    \n",
        "    return env_asset"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735204453074
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = get_or_create_docker_environment_asset(ml_client, azure_env_name, docker_dir=f\"{CLOUD_DIR}/train\", update=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Exception: Found Environment asset, but will update the Environment.\nCreated Environment asset: slm-llama-acft-custom-env\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading train (0.01 MBs):   0%|          | 0/12889 [00:00<?, ?it/s]\r\u001b[32mUploading train (0.01 MBs): 100%|██████████| 12889/12889 [00:00<00:00, 264405.67it/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735210065403
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Training\n",
        "#### 3.1 Create the compute cluster"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "### Create the compute cluster\n",
        "try:\n",
        "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
        "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
        "except Exception as ex:\n",
        "    print(\n",
        "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
        "    )\n",
        "    try:\n",
        "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
        "        tier = 'LowPriority' if USE_LOWPRIORITY_VM else 'Dedicated'\n",
        "        compute = AmlCompute(\n",
        "            name=azure_compute_cluster_name,\n",
        "            size=azure_compute_cluster_size,\n",
        "            tier=tier,\n",
        "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
        "        )\n",
        "        ml_client.compute.begin_create_or_update(compute).wait()\n",
        "    except Exception as e:\n",
        "        print(\"Error\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "The compute cluster already exists! Reusing it for the current run\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735214697703
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3.2 Start the training job\n",
        "\n",
        "The `command` allows user to configure the following key aspects.  \n",
        "  \n",
        "- **inputs** - This is the dictionary of inputs using name value pairs to the command.  \n",
        "  - **type** - The type of input. This can be a `uri_file` or `uri_folder`. The default is `uri_folder`.  \n",
        "  - **path** - The path to the file or folder. These can be local or remote files or folders. For remote files - http/https, wasb are supported.  \n",
        "    - Azure ML `data`/`dataset` or `datastore` are of type `uri_folder`. To use `data`/`dataset` as input, you can use registered dataset in the workspace using the format `'<data_name>:<version>'`. For example, `Input(type='uri_folder', path='my_dataset:1')`  \n",
        "  - **mode** - Mode of how the data should be delivered to the compute target. Allowed values are `ro_mount`, `rw_mount`, and `download`. Default is `ro_mount`.  \n",
        "  \n",
        "- **code** - This is the path where the code to run the command is located.  \n",
        "  \n",
        "- **compute** - The compute on which the command will run. You can run it on the local machine by using `local` for the compute.  \n",
        "  \n",
        "- **command** - This is the command that needs to be run using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, we can use the `Input` class. The `Input` class supports three parameters:  \n",
        "  \n",
        "- **environment** - This is the environment needed for the command to run. Curated (built-in) or custom environments from the workspace can be used.  \n",
        "  \n",
        "- **instance_count** - Number of nodes. Default is 1.  \n",
        "  \n",
        "- **distribution** - Distribution configuration for distributed training scenarios. Azure Machine Learning supports PyTorch, TensorFlow, and MPI-based distributed.  \n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.entities import ResourceConfiguration\n",
        "\n",
        "job = command(\n",
        "    inputs=dict(\n",
        "        #train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
        "        train_dir=Input(path=f\"{AZURE_DATA_NAME}_train@latest\"),  # Get data from Data asset\n",
        "        val_dir = Input(path=f\"{AZURE_DATA_NAME}_val@latest\"),\n",
        "        epoch=d['train']['epoch'],\n",
        "        train_batch_size=d['train']['train_batch_size'],\n",
        "        eval_batch_size=d['train']['eval_batch_size'],  \n",
        "    ),\n",
        "    code=f\"{CLOUD_DIR}/train\",  # local path where the code is stored\n",
        "    compute=azure_compute_cluster_name,\n",
        "    command=\"python train.py --train_dir ${{inputs.train_dir}} --val_dir ${{inputs.val_dir}} --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}}\",\n",
        "    #environment=\"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/77\", # Use built-in Environment asset\n",
        "    environment=f\"{azure_env_name}@latest\",\n",
        "    distribution={\n",
        "        \"type\": \"PyTorch\",\n",
        "        \"process_count_per_instance\": 1, # For multi-gpu training set this to an integer value more than 1\n",
        "    },\n",
        ")\n",
        "returned_job = ml_client.jobs.create_or_update(job)\n",
        "ml_client.jobs.stream(returned_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\r\u001b[32mUploading train (0.01 MBs):   0%|          | 0/12912 [00:00<?, ?it/s]\r\u001b[32mUploading train (0.01 MBs): 100%|██████████| 12912/12912 [00:00<00:00, 240729.22it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RunId: amiable_pea_f8lw9wb6mg\nWeb View: https://ml.azure.com/runs/amiable_pea_f8lw9wb6mg?wsid=/subscriptions/8cebb108-a4d5-402b-a0c4-f7556126277f/resourcegroups/azure-ml-priya-demo/workspaces/azure-ml-priya-westus3\n\nExecution Summary\n=================\nRunId: amiable_pea_f8lw9wb6mg\nWeb View: https://ml.azure.com/runs/amiable_pea_f8lw9wb6mg?wsid=/subscriptions/8cebb108-a4d5-402b-a0c4-f7556126277f/resourcegroups/azure-ml-priya-demo/workspaces/azure-ml-priya-westus3\n\nWarnings:\nAzureMLCompute job failed\nExecutionFailed: [REDACTED]\n\texit_codes: 1\n\tAppinsights Reachable: Some(true)\n\n"
        },
        {
          "output_type": "error",
          "ename": "JobException",
          "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process 'Rank 0' exited with status code 1. Please check log file 'user_logs/std_log_process_0.txt' for error details. Error: [rank0]:   File \\\"/mnt/azureml/cr/j/5b1ff9c45f074fd2b11bcf2dbbfa477b/exe/wd/train.py\\\", line 180, in main\\n[rank0]:     train_dataset, eval_dataset = prepare_dataset(tokenizer, args)\\n[rank0]:   File \\\"/mnt/azureml/cr/j/5b1ff9c45f074fd2b11bcf2dbbfa477b/exe/wd/train.py\\\", line 90, in prepare_dataset\\n[rank0]:     train_dataset = train_dataset.map(apply_chat_template, batched = True, remove_columns=column_names)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 560, in wrapper\\n[rank0]:     out: Union[\\\"Dataset\\\", \\\"DatasetDict\\\"] = func(self, *args, **kwargs)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 3055, in map\\n[rank0]:     for rank, done, content in Dataset._map_single(**dataset_kwargs):\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 3470, in _map_single\\n[rank0]:     buf_writer, writer, tmp_file = init_buffer_and_writer()\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 3392, in init_buffer_and_writer\\n[rank0]:     tmp_file = tempfile.NamedTemporaryFile(\\\"wb\\\", dir=cache_dir, delete=False)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/tempfile.py\\\", line 575, in NamedTemporaryFile\\n[rank0]:     file = _io.open(dir, mode, buffering=buffering,\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/tempfile.py\\\", line 572, in opener\\n[rank0]:     fd, name = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/tempfile.py\\\", line 256, in _mkstemp_inner\\n[rank0]:     fd = _os.open(file, flags, 0o600)\\n[rank0]: OSError: [Errno 30] Read-only file system: '/mnt/azureml/cr/j/5b1ff9c45f074fd2b11bcf2dbbfa477b/cap/data-capability/wd/INPUT_train_dir/tmpshbimem3'\\n[rank0]:[W1226 13:14:34.701011626 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\\n\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"component_name\": \"CommonRuntime\"\n} ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 25\u001b[0m\n\u001b[1;32m      5\u001b[0m job \u001b[38;5;241m=\u001b[39m command(\n\u001b[1;32m      6\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m#train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     },\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m returned_job \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(job)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturned_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:818\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:334\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    332\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    335\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    336\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    337\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    339\u001b[0m         )\n\u001b[1;32m    341\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
            "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process 'Rank 0' exited with status code 1. Please check log file 'user_logs/std_log_process_0.txt' for error details. Error: [rank0]:   File \\\"/mnt/azureml/cr/j/5b1ff9c45f074fd2b11bcf2dbbfa477b/exe/wd/train.py\\\", line 180, in main\\n[rank0]:     train_dataset, eval_dataset = prepare_dataset(tokenizer, args)\\n[rank0]:   File \\\"/mnt/azureml/cr/j/5b1ff9c45f074fd2b11bcf2dbbfa477b/exe/wd/train.py\\\", line 90, in prepare_dataset\\n[rank0]:     train_dataset = train_dataset.map(apply_chat_template, batched = True, remove_columns=column_names)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 560, in wrapper\\n[rank0]:     out: Union[\\\"Dataset\\\", \\\"DatasetDict\\\"] = func(self, *args, **kwargs)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 3055, in map\\n[rank0]:     for rank, done, content in Dataset._map_single(**dataset_kwargs):\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 3470, in _map_single\\n[rank0]:     buf_writer, writer, tmp_file = init_buffer_and_writer()\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/site-packages/datasets/arrow_dataset.py\\\", line 3392, in init_buffer_and_writer\\n[rank0]:     tmp_file = tempfile.NamedTemporaryFile(\\\"wb\\\", dir=cache_dir, delete=False)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/tempfile.py\\\", line 575, in NamedTemporaryFile\\n[rank0]:     file = _io.open(dir, mode, buffering=buffering,\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/tempfile.py\\\", line 572, in opener\\n[rank0]:     fd, name = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\\n[rank0]:   File \\\"/opt/conda/envs/ptca/lib/python3.10/tempfile.py\\\", line 256, in _mkstemp_inner\\n[rank0]:     fd = _os.open(file, flags, 0o600)\\n[rank0]: OSError: [Errno 30] Read-only file system: '/mnt/azureml/cr/j/5b1ff9c45f074fd2b11bcf2dbbfa477b/cap/data-capability/wd/INPUT_train_dir/tmpshbimem3'\\n[rank0]:[W1226 13:14:34.701011626 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())\\n\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"component_name\": \"CommonRuntime\"\n} "
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1735220171323
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Register the model for future deployment and inference"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Run \n",
        "import os  \n",
        "  \n",
        "# Connect to your workspace  \n",
        "ws = Workspace.from_config()  \n",
        "  \n",
        "experiment_name =  'fine-tune-llama-32b-fc'\n",
        "run_id = 'amiable_pea_f8lw9wb6mg'\n",
        "run = Run(ws.experiments[experiment_name], run_id)  \n",
        "\n",
        "# Register the model  \n",
        "model = run.register_model(  \n",
        "    model_name=d[\"serve\"][\"azure_model_name\"],  # this is the name the model will be registered under  \n",
        "    model_path=\"outputs\"  # this is the path to the model file in the run's outputs  \n",
        ")  \n",
        "# Create a local directory to save the outputs  \n",
        "local_folder = './model'  \n",
        "os.makedirs(local_folder, exist_ok=True)  \n",
        "  \n",
        "# Download the entire outputs folder  \n",
        "run.download_files(prefix='outputs', output_directory=local_folder)  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}