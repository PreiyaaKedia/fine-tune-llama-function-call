{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 1. Serving Preparation\n",
        "##### \n",
        "##### 1.1 Configure Workspace details\n",
        "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1736318772420
        }
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "# sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
        "\n",
        "config_file = \"/llama-fc-wo-descriptions_config.yaml\"\n",
        "\n",
        "with open(f'./{config_file}') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_SFT_DATA_NAME']    \n",
        "DATA_DIR = d['config']['SFT_DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
        "IS_DEBUG = d['config']['IS_DEBUG']\n",
        "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
        "\n",
        "azure_env_name = d['serve']['azure_env_name']\n",
        "azure_compute_cluster_name = d['serve']['azure_compute_cluster_name']\n",
        "azure_compute_cluster_size = d['serve']['azure_serving_cluster_size']\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CLOUD_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1736318774033
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create console handler with a higher log level\n",
        "console_handler = logging.StreamHandler()\n",
        "console_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create file handler which logs even debug messages\n",
        "file_handler = logging.FileHandler(\"debug.log\")\n",
        "file_handler.setLevel(logging.DEBUG)  # Set this to the lowest level you want to capture\n",
        "\n",
        "# Create formatter and add it to the handlers\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "console_handler.setFormatter(formatter)\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the handlers to the logger\n",
        "logger.addHandler(console_handler)\n",
        "logger.addHandler(file_handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1736318783114
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-08 06:46:23,913 - __main__ - INFO - ===== 0. Azure ML Training Info =====\n",
            "2025-01-08 06:46:23,914 - __main__ - INFO - AZURE_SUBSCRIPTION_ID=8cebb108-a4d5-402b-a0c4-f7556126277f\n",
            "2025-01-08 06:46:23,915 - __main__ - INFO - AZURE_RESOURCE_GROUP=azure-ml-priya-demo\n",
            "2025-01-08 06:46:23,916 - __main__ - INFO - AZURE_WORKSPACE=azure-ml-priya-westus3\n",
            "2025-01-08 06:46:23,918 - __main__ - INFO - AZURE_DATA_NAME=sft-data-function-call-wo-desc\n",
            "2025-01-08 06:46:23,918 - __main__ - INFO - DATA_DIR=./dataset_wo_desc\n",
            "2025-01-08 06:46:23,920 - __main__ - INFO - CLOUD_DIR=./cloud\n",
            "2025-01-08 06:46:23,920 - __main__ - INFO - HF_MODEL_NAME_OR_PATH=unsloth/Llama-3.2-3B-Instruct\n",
            "2025-01-08 06:46:23,921 - __main__ - INFO - IS_DEBUG=True\n",
            "2025-01-08 06:46:23,922 - __main__ - INFO - USE_LOWPRIORITY_VM=False\n",
            "2025-01-08 06:46:23,923 - __main__ - INFO - azure_env_name=slm-serving-llama\n",
            "2025-01-08 06:46:23,924 - __main__ - INFO - azure_compute_cluster_name=gpu-a100-demo-vm\n",
            "2025-01-08 06:46:23,926 - __main__ - INFO - azure_compute_cluster_size=Standard_NC24ads_A100_v4\n"
          ]
        }
      ],
      "source": [
        "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
        "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
        "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
        "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
        "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
        "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
        "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
        "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
        "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
        "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
        "logger.info(f\"azure_env_name={azure_env_name}\")\n",
        "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
        "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1736318795035
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found the config file in: /config.json\n"
          ]
        }
      ],
      "source": [
        "# import required libraries\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### 1.2 Create Model asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1736318812155
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_or_create_model_asset(ml_client, model_name, job_name, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
        "    \n",
        "    try:\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
        "        else:\n",
        "            latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
        "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
        "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        logger.info(f\"Exception: {e}\")        \n",
        "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"    \n",
        "        run_model = Model(\n",
        "            name=model_name,        \n",
        "            path=model_path,\n",
        "            description=\"Model created from run.\",\n",
        "            type=model_type # mlflow_model, custom_model, triton_model\n",
        "        )\n",
        "        model_asset = ml_client.models.create_or_update(run_model)\n",
        "        logger.info(f\"Created Model asset: {model_name}\")\n",
        "\n",
        "    return model_asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1736318817753
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-01-08 06:46:58,633 - __main__ - INFO - Found Model asset: llama-fc-wo-descriptions-ft. Will not create again\n"
          ]
        }
      ],
      "source": [
        "model_dir = d['train']['model_dir']\n",
        "model = get_or_create_model_asset(ml_client, d[\"serve\"][\"azure_model_name\"], \"<job_name>\", model_dir, model_type=\"custom_model\", update=False)\n",
        "\n",
        "# model = get_or_create_model_asset(ml_client, d['serve']['azure_model_name'], update = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 1.3 Create AzureML environment\n",
        "####\n",
        "Azure ML defines containers (called environment asset) in which your code will run. You can use a pre-built enviornment or create a custom enviornment. For this hands-on session, we will buid a custom Docker enviornment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ./cloud/serve/Dockerfile\n"
          ]
        }
      ],
      "source": [
        "%%writefile {CLOUD_DIR}/serve/Dockerfile\n",
        "\n",
        "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
        "\n",
        "# Install pip dependencies\n",
        "COPY requirements.txt .\n",
        "RUN pip install -r requirements.txt --no-cache-dir\n",
        "\n",
        "# Inference requirements\n",
        "COPY --from=mcr.microsoft.com/azureml/o16n-base/python-assets:20230419.v1 /artifacts /var/\n",
        "\n",
        "RUN /var/requirements/install_system_requirements.sh && \\\n",
        "    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\n",
        "    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\n",
        "    ln -sf /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\n",
        "    rm -f /etc/nginx/sites-enabled/default\n",
        "ENV SVDIR=/var/runit\n",
        "ENV WORKER_TIMEOUT=400\n",
        "EXPOSE 5001 8883 8888\n",
        "\n",
        "# support Deepspeed launcher requirement of passwordless ssh login\n",
        "RUN apt-get update\n",
        "RUN apt-get install -y openssh-server openssh-client\n",
        "\n",
        "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing ./cloud/serve/requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile {CLOUD_DIR}/serve/requirements.txt\n",
        "azureml-core==1.58.0\n",
        "azureml-dataset-runtime==1.58.0\n",
        "azureml-defaults==1.58.0\n",
        "azure-ml==0.0.1\n",
        "azure-ml-component==0.9.18.post2\n",
        "azureml-mlflow==1.58.0\n",
        "azureml-contrib-services==1.58.0\n",
        "azureml-contrib-services==1.58.0\n",
        "azureml-automl-common-tools==1.58.0\n",
        "torch-tb-profiler==0.4.3\n",
        "azureml-inference-server-http~=1.3\n",
        "inference-schema==1.8.0\n",
        "MarkupSafe==3.0.2\n",
        "regex\n",
        "pybind11\n",
        "bitsandbytes==0.44.1\n",
        "transformers==4.46.1\n",
        "peft==0.13.2\n",
        "accelerate==1.1.0\n",
        "datasets\n",
        "scipy\n",
        "azure-identity\n",
        "packaging==24.1\n",
        "timm==1.0.11\n",
        "einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1736318822399
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found Environment asset: slm-serving-florence. Will not create again\n"
          ]
        }
      ],
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
        "    \n",
        "    try:\n",
        "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
        "        if update:\n",
        "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
        "        else:\n",
        "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
        "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
        "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
        "        print(f\"Exception: {e}\")\n",
        "        env_docker_image = Environment(\n",
        "            build=BuildContext(path=docker_dir),\n",
        "            name=env_name,\n",
        "            description=\"Environment created from a Docker context.\",\n",
        "        )\n",
        "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
        "        print(f\"Created Environment asset: {env_name}\")\n",
        "    \n",
        "    return env_asset\n",
        "\n",
        "env = get_or_create_docker_environment_asset(ml_client, \"slm-serving-florence\", f\"{CLOUD_DIR}/inference\", update=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 1.4 Serving script\n",
        "####\n",
        "If you are not serving using a MLflow model but instead using a custom model, you can write your own script. This step demosntrates how to write the scoring script to run the inference.\n",
        "\n",
        "The scoring script consists of two components:\n",
        "\n",
        "1. init() : This is where you define the global initialization logic like loading of LLM models and tokenizers\n",
        "2. run() : Inference logic called for every invocation of the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./cloud/inference/score.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {CLOUD_DIR}/serve/score.py\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import base64\n",
        "import logging\n",
        "\n",
        "from io import BytesIO\n",
        "from transformers import AutoTokenizer, AutoProcessor, pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def init():\n",
        "    \"\"\"\n",
        "    This function is called when the container is initialized/started, typically after create/update of the deployment.\n",
        "    You can write the logic here to perform init operations like caching the model in memory\n",
        "    \"\"\"\n",
        "    global model\n",
        "    global tokenizer\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # Please provide your model's folder name if there is one\n",
        "    model_name_or_path = os.path.join(\n",
        "        os.getenv(\"AZUREML_MODEL_DIR\"), \"{{score_model_dir}}\"\n",
        "    )\n",
        "    \n",
        "    model_kwargs = dict(\n",
        "        trust_remote_code=True,    \n",
        "        device_map={\"\":0},\n",
        "        torch_dtype=\"auto\" \n",
        "    )\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)    \n",
        "\n",
        "    logging.info(\"Loaded model.\")\n",
        "    \n",
        "def run(json_data: str):\n",
        "    logging.info(\"Request received\")\n",
        "    data = json.loads(json_data)\n",
        "    input_data = data[\"input_data\"]\n",
        "    params = data['params']\n",
        "\n",
        "    # pipe = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
        "    # output = pipe(input_data, **params)\n",
        "    # result = output[0][\"generated_text\"]\n",
        "    # logging.info(f\"Generated text : {result}\")\n",
        "    inputs = tokenizer.apply_chat_template(input_data, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, do_sample = True, temperature = 0.1)\n",
        "    result = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens = True)\n",
        "\n",
        "    json_result = {\"result\" : result}\n",
        "\n",
        "    return json_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Plug in the appropriate variable in the inference script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1736163391659
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1986"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jinja2\n",
        "from pathlib import Path\n",
        "TRAINED_MLFLOW = False\n",
        "\n",
        "jinja_env = jinja2.Environment()  \n",
        "\n",
        "template = jinja_env.from_string(Path(f\"{CLOUD_DIR}/inference/score.py\").open().read())\n",
        "score_model_dir = model_dir.split(\"/\")[-1]\n",
        "\n",
        "Path(f\"{CLOUD_DIR}/inference/score.py\").open(\"w\").write(\n",
        "    template.render(score_model_dir=score_model_dir)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 2. Serving\n",
        "#####\n",
        "##### 2.1 Create the endpoint\n",
        "Online endpoints give a durable REST API that can be used to integrate with applications that need to use the model.\n",
        " \n",
        "Note : This step doesn't provision the GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---Endpoint created successfully---\n",
            "\n",
            "CPU times: user 70.3 ms, sys: 9.14 ms, total: 79.4 ms\n",
            "Wall time: 1min 33s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    IdentityConfiguration,\n",
        "    ManagedIdentityConfiguration,\n",
        ")\n",
        "\n",
        "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
        "# Check if the endpoint already exists in the workspace\n",
        "try:\n",
        "    endpoint = ml_client.online_endpoints.get(azure_endpoint_name)\n",
        "    print(\"---Endpoint already exists---\")\n",
        "except:\n",
        "    # Create an online endpoint if it doesn't exist\n",
        "\n",
        "    # Define the endpoint\n",
        "    endpoint = ManagedOnlineEndpoint(\n",
        "        name=azure_endpoint_name,\n",
        "        description=f\"Test endpoint for {model.name}\",\n",
        "    )\n",
        "\n",
        "# Trigger the endpoint creation\n",
        "try:\n",
        "    ml_client.begin_create_or_update(endpoint).wait()\n",
        "    print(\"\\n---Endpoint created successfully---\\n\")\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Endpoint creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3.2 Create the Deployment\n",
        "This process takes lot of time as GPU clusters needs to be provisioned and serving environment must be built"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint llama-endpoint-ft exists\n",
            "\u001b[32mUploading inference (0.0 MBs): 100%|██████████| 2301/2301 [00:00<00:00, 12618.12it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "......................................................................................................."
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from azure.ai.ml.entities import (    \n",
        "    OnlineRequestSettings,\n",
        "    CodeConfiguration,\n",
        "    ManagedOnlineDeployment,\n",
        "    ProbeSettings,\n",
        "    Environment\n",
        ")\n",
        "\n",
        "azure_deployment_name = f\"{d['serve']['azure_deployment_name']}\"\n",
        "\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=azure_deployment_name,\n",
        "    endpoint_name=azure_endpoint_name,\n",
        "    model=model,\n",
        "    instance_type=azure_compute_cluster_size,\n",
        "    instance_count=1,\n",
        "    #code_configuration=code_configuration,\n",
        "    environment = env,\n",
        "    scoring_script=\"score.py\",\n",
        "    code_path=f\"./{CLOUD_DIR}/serve\",\n",
        "    #environment_variables=deployment_env_vars,\n",
        "    request_settings=OnlineRequestSettings(max_concurrent_requests_per_instance=50,\n",
        "                                           request_timeout_ms=90000, max_queue_wait_ms=60000),\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        period=100,\n",
        "        initial_delay=500,\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        period=100,\n",
        "        initial_delay=500,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Trigger the deployment creation\n",
        "try:\n",
        "    ml_client.begin_create_or_update(deployment).wait()\n",
        "    print(\"\\n---Deployment created successfully---\\n\")\n",
        "except Exception as err:\n",
        "    raise RuntimeError(\n",
        "        f\"Deployment creation failed. Detailed Response:\\n{err}\"\n",
        "    ) from err\n",
        "    \n",
        "endpoint.traffic = {azure_deployment_name: 100}\n",
        "endpoint_poller = ml_client.online_endpoints.begin_create_or_update(endpoint)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### 3. Inference\n",
        "##### Test invocation\n",
        "Run inference on managed endpoint using sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1736166092725
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os \n",
        "\n",
        "sample = {\n",
        "    \"input_data\": \n",
        "        [\n",
        "            {'role': 'system', 'content': 'You are an helpful assistant who has access to the following functions to help the user, you can use the functions if needed- { \"name\": \"calculate_shipping_cost\", \"description\": \"Calculate the cost of shipping a package\", \"parameters\": { \"type\": \"object\", \"properties\": { \"weight\": { \"type\": \"number\", \"description\": \"The weight of the package in pounds\" }, \"destination\": { \"type\": \"string\", \"description\": \"The destination of the package\" } }, \"required\": [ \"weight\", \"destination\" ] }}}\"'},\n",
        "            {'role': 'user', 'content': 'Can you help me with shipping cost for a package?'},\n",
        "            {'role': 'assistant', 'content': 'Sure! I can help you with that. Please provide me with the weight and destination of the package.'},\n",
        "            {'role': 'user', 'content': 'The weight of the package is 10 pounds and the destination is New York.'}\n",
        "        ],\n",
        "    \"params\": {\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"do_sample\": True,\n",
        "        \"return_full_text\": False\n",
        "    }\n",
        "}\n",
        "\n",
        "# sample = {\n",
        "#     \"input_data\": \n",
        "#         [\n",
        "#             {\"role\": \"user\", \"content\": \"Tell me Microsoft's brief history.\"},\n",
        "#             {\"role\": \"assistant\", \"content\": \"Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell a BASIC interpreter for the Altair 8800.\"},\n",
        "#             {\"role\": \"user\", \"content\": \"What about Amazon's history?\"}\n",
        "#         ],\n",
        "#     \"params\": {\n",
        "#         \"temperature\": 0.1,\n",
        "#         \"max_new_tokens\": 128,\n",
        "#         \"do_sample\": True,\n",
        "#         \"return_full_text\": False\n",
        "#     }\n",
        "# }\n",
        "\n",
        "test_inference_dir = \"./inference\"\n",
        "os.makedirs(test_inference_dir, exist_ok=True)\n",
        "\n",
        "request_file = os.path.join(test_inference_dir, \"sample_request.json\")\n",
        "\n",
        "with open(request_file, \"w\") as f:\n",
        "    json.dump(sample, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1736171110576
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'tool_uses': [{'recipient_name': 'functions.calculate_shipping_cost', 'parameters': {'weight': 10, 'destination': 'New York'}}]}\n"
          ]
        }
      ],
      "source": [
        "azure_endpoint_name = d['serve']['azure_endpoint_name']\n",
        "azure_deployment_name = f\"{d['serve']['azure_deployment_name']}\"\n",
        "\n",
        "result = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=azure_endpoint_name,\n",
        "    deployment_name=azure_deployment_name,\n",
        "    request_file=request_file\n",
        ")\n",
        "\n",
        "result_json = json.loads(result)\n",
        "result = result_json['result']\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### "
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
